---
layout: post
title: TTS
tags: [TTS, Transformer, PyTorch]
excerpt_separator: <!--more-->
use_math: true
---

처음부터 다중화자 TTS를 향해 달려가면 조금 지칠 수 있으니 처음에는 Transformer를 이용하여 단일화자 TTS를 만드는 것을 1차 목표로 하고 진행한다.

Transformer로 챗봇 만들어봤으니 **이제 스크립트-음성 짝지어서 그대로 데이터만 바꾸면 TTS 만들 수 있겠다.** 라고 착각하기에는 몇가지 큰 걸림돌들이 있다.  
그 중 가장 큰 차이는

- **음성 데이터는 정수 sequence로 주어지지 않는다.**

기존의 챗봇을 생각해보면 transformer의 마지막은 다음과 같다.  <!--more-->
Decoder의 output이 출력 dim을 target vocab_size로 가지는 Linear를 거친 뒤 softmax를 취해 가장 확률값이 높은 token을 해당 sequence의 token으로 한다.

하지만 음성은 어떨까? 음성도 단어를 tokenize하듯이 음소 하나하나를 token 하나하나로 mapping할 수 있을까? 정답은 당연히 '아니다'이다.

그럼 음성은 어떻게 input과 output으로 활용할까?
이를 알기 위해서는 오디오 처리에 대한 지식이 조금 필요하다.

## Audio Data

대표적인 오디오 처리 라이브러리인 librosa로 wav파일을 불러오면 다음과 같다.

```python
y, _ = librosa.load(ORIGIN_FILE_PATH, sr=22050)

print('y.shape:', y.shape)
print(y)
plt.plot(y)
plt.show()
```

![wave_image](/assets/img/posts/TTS_1/wav_image.png)

우리가 흔히 알고있는 오디오의 모습이다.

이를 그대로 이용할 수 있을까? 사실 하려면야 못할 것은 없겠지만 걸림돌이 굉장히 많다.  
그 중 가장 큰 걸림돌은 sequence의 길이이다. 위 이미지는 3초가 채 되지 않는 오디오이다. 하지만 sequence의 길이는 50000이 넘는다.  
이를 감당하기는 쉽지 않다. 참고로 이전 챗봇에서 sequence의 최대길이는 50이었다.

따라서 이 sequence의 길이를 줄이면서도 원래의 오디오정보는 잃지 않는 일종의 압축 기법이 필요하다. 여기에 흔히 쓰이는 것이 **Mel Spectrogram**이다.

### Mel Spectrogram

우선 스펙트로그램이란 무엇인지 알아보자.

>스펙트로그램(Spectrogram)은 소리나 파동을 시각화하여 파악하기 위한 도구로, 파형(waveform)과 스펙트럼(spectrum)의 특징이 조합되어 있다.  
>파형에서는 시간축의 변화에 따른 진폭 축의 변화를 볼 수 있고, 스펙트럼에서는 주파수 축의 변화에 따른 진폭 축의 변화를 볼 수 있는 반면, 스펙트로그램에서는 시간축과 주파수 축의 변화에 따라 진폭의 차이를 인쇄 농도 / 표시 색상의 차이로 나타낸다.  
[출처 wikipedia](https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%8E%99%ED%8A%B8%EB%A1%9C%EA%B7%B8%EB%9E%A8)

쉽게 말하면 X축은 시간(Time), Y축은 주파수(Hz), 색깔은 소리의 크기(dB)라는 얘기다.

그리고 이중에서도 Mel Spectrogram이란 Spectrogram에 Mel-filter를 적용해서 얻은 결과물로 사람의 청각기관이 고주파보다는 저주파에 민감한 특징을 이용하여 사람이 들을 수 있는 정보만을 골라낸 것이라고 생각하면 편하다. (자세한 설명은 [여기](https://hyunlee103.tistory.com/46?category=999732)를 참고하도록 하자.)

앞서 보여준 오디오를 스펙트로그램으로 바꿔서 시각화한 결과를 보자.

![spectrogram](/assets/img/posts/TTS_1/spectrogram.png)

그림을 보면 직관적으로 이해 가능하다.
