---
layout: post
title: TTS - 오디오 처리
tags: [TTS, Transformer, PyTorch]
excerpt_separator: <!--more-->
use_math: true
---

처음부터 다중화자 TTS를 향해 달려가면 조금 지칠 수 있으니 처음에는 Transformer를 이용하여 단일화자 TTS를 만드는 것을 1차 목표로 하고 진행한다.

Transformer로 챗봇 만들어봤으니 **이제 스크립트-음성 짝지어서 그대로 데이터만 바꾸면 TTS 만들 수 있겠다.** 라고 착각하기에는 몇가지 큰 걸림돌들이 있다.  
그 중 가장 큰 차이는

- **음성 데이터는 정수 sequence로 주어지지 않는다.**

기존의 챗봇을 생각해보면 transformer의 마지막은 다음과 같다.  <!--more-->
Decoder의 output이 출력 dim을 target vocab_size로 가지는 Linear를 거친 뒤 softmax를 취해 가장 확률값이 높은 token을 해당 sequence의 token으로 한다.

하지만 음성은 어떨까? 음성도 단어를 tokenize하듯이 음소 하나하나를 token 하나하나로 mapping할 수 있을까? 정답은 당연히 '아니다'이다.

그럼 음성은 어떻게 input과 output으로 활용할까?

## Architecture

우선 현재 참고하고있는 논문인 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)에서 제안하는 아키텍쳐를 살펴보자

![transformer_tts_architecture](/assets/img/posts/TTS_1/transformer_tts_architecture.png)

디코더의 input으로 들어오는 오디오 모양도 그렇고 output의 모양도 그렇고 우리가 흔히 아는 음파 파동의 형태가 아니다. 이게 뭘까.

이를 알기 위해서는 오디오 처리에 대한 지식이 조금 필요하다.

## Audio Data

대표적인 오디오 처리 라이브러리인 librosa로 wav파일을 불러오면 다음과 같다.

```python
y, _ = librosa.load(ORIGIN_FILE_PATH, sr=22050)

print('y.shape:', y.shape)
print(y)
plt.plot(y)
plt.show()
```

![wave_image](/assets/img/posts/TTS_1/wav_image.png)

우리가 흔히 알고있는 오디오의 모습이다.

이를 그대로 이용할 수 있을까? 사실 하려면야 못할 것은 없겠지만 걸림돌이 굉장히 많다.  
그 중 가장 큰 걸림돌은 sequence의 길이이다. 위 이미지는 3초가 채 되지 않는 오디오이다. 하지만 sequence의 길이는 50000이 넘는다.  
이를 감당하기는 쉽지 않다. 참고로 이전 챗봇에서 sequence의 최대길이는 50이었다.

따라서 이 sequence의 길이를 줄이면서도 원래의 오디오정보는 잃지 않는 일종의 압축 기법이 필요하다. 여기에 흔히 쓰이는 것이 **Mel Spectrogram**이다.

### Mel Spectrogram

우선 스펙트로그램이란 무엇인지 알아보자.

>스펙트로그램(Spectrogram)은 소리나 파동을 시각화하여 파악하기 위한 도구로, 파형(waveform)과 스펙트럼(spectrum)의 특징이 조합되어 있다.  
>파형에서는 시간축의 변화에 따른 진폭 축의 변화를 볼 수 있고, 스펙트럼에서는 주파수 축의 변화에 따른 진폭 축의 변화를 볼 수 있는 반면, 스펙트로그램에서는 시간축과 주파수 축의 변화에 따라 진폭의 차이를 인쇄 농도 / 표시 색상의 차이로 나타낸다.  
[출처 wikipedia](https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%8E%99%ED%8A%B8%EB%A1%9C%EA%B7%B8%EB%9E%A8)

쉽게 말하면 X축은 시간(Time), Y축은 주파수(Hz), 색깔은 소리의 크기(dB)라는 얘기다.

그리고 이중에서도 Mel Spectrogram이란 Spectrogram에 Mel-filter를 적용해서 얻은 결과물로 사람의 청각기관이 고주파보다는 저주파에 민감한 특징을 이용하여 사람이 들을 수 있는 정보만을 골라낸 것이라고 생각하면 편하다. (자세한 설명은 여기 [현토리님의 블로그](https://hyunlee103.tistory.com/46?category=999732)를 참고하도록 하자.)

앞서 보여준 오디오를 스펙트로그램으로 바꿔서 시각화한 결과를 보자.

![spectrogram](/assets/img/posts/TTS_1/spectrogram.png)

그림을 보면 직관적으로 이해 가능하다.

### How To Convert

이제 원래의 `wav`파일을 `mel spectrogram`으로 바꾸는 방법에 대해 알아보자.  
코드부터 보자면 다음과 같다.

```python
def load_wav(fpath, sr):
    wav, _ = librosa.load(fpath, sr=sr)
    return wav

def get_mel(fpath, sr, n_mels, n_fft, win_length, hop_length):

    y = load_wav(fpath, sr=sr)
    
    mel = librosa.feature.melspectrogram(
        y=y, n_mels=n_mels, n_fft=n_fft,
        hop_length=hop_length, win_length=win_length)
    
    return mel.T


wav = load_wav(audio_file_path, 22050)
mel = get_mel(audio_file_path, 22050, 80, 2048, int(22050*0.05), int(22050*0.0125))

print(f'wav.shape: {wav.shape}')
print(f'mel.shape: {mel.shape}')  # (seq_len, n_mels)
```

```
wav.shape: (53358,)
mel.shape: (195, 80)
```

우선 `wav`파일을 불러오는 부분을 보면 `sr`이라는 parameter가 필요하다.

- `sr`: sample rate의 약자로 초당 수집하는 디지털 신호를 의미한다. 본래 아날로그 신호인 공기의 진동을 초당 하나의 숫자로 모으는 횟수라고 생각하면 된다. 그리고 이는 오디오 파일이 고유로 가진 수치이므로 미리 확인이 필요하다.  
즉 현재 불러온 오디오 파일의 길이(시간 기준)는 53358/22050(=2.420초)이다.

이제 `wav`파일을 `mel spectrogram`으로 변환하기 위해서는 몇가지 parameter 설정이 필요하다.  
(다음 설명들은 모두 쉽게 이해하기 위한 설명으로 완벽한 설명은 아니다. 또한 이 외에도 다른 parameter들이 존재하지만 사용하는 것만 설명하도록 한다.)

- `n_mels`: 사람이 듣는 주파수를 몇단계로 분리할 것인지를 결정한다. 다른말로 설명하자면 위 mel spectrogram 시각화의 row 개수라고 생각하면 된다.
- `n_fft`: 
- `win_length`: `wav`파일의 몇칸을 `mel spectrogram`에서 한칸으로 만들것인가를 결정한다. 다만 인간이 인지하기 쉬운 time의 영역이 아니라 디지털 신호의 개수를 의미하므로 보통 time영역에서 `frame_length`를 결정하고 `win_length = int(sr*frame_length)`로 사용하는 경우가 많다.
- `hop_length`: `wav`파일에서 몇칸씩 띄어가며 window를 만들지 결정한다. 보통 `win_length`보다 짧게 설정하며, `win_length`보다 `hop_length`가 짧으면 smoothing의 효과가 있다. `win_length`와 마찬가지로 디지털 신호 자체의 영역에서 정의하기보단, time영역에서 `frame_shift`를 결정하고 `hop_length = int(sr*frame_shift)`로 사용하는 경우가 많다.

변환 결과를 보면 sequence의 길이가 53358에서 195로 짧아졌다. 엄청난 효과이다.

### How To Predict Mel Spectrogram

가장 큰 걸림돌이었던 **너무 긴 sequence의 길이**를 해결했다. 그럼 이제 이것을 예측하면 된다!

라고 말해도 사실 잘 모르겠다.  
왜냐하면 기존 Transformer는 기본적으로 현재까지의 sequence 다음에 등장할 token의 확률을 softmax를 통해 output으로 주는 구조였다. 그런데 현재 53358에서 195로 sequence의 길이를 줄였다 한들 mel spectrogram 한개가 token인가? 아니다. 그럼 이는 어떻게 예측해야할까?  
사실 생각보다 매우 간단하다.

일단은 기존 Transformer의 아키텍쳐를 보자.

![transformer_architecture](/assets/img/posts/TTS_1/transformer_architecture.png)

주목해야할 부분은 마지막의 `Linear`와 `Softmax`이다. 이 둘을 통과하기 전 Decoder Stack에서 나온 데이터는 어떤 shape을 가지고 있을까?  
`(batch_size, target_len(query_len), hidden_dim)`이다.  

그리고 우리가 예측해야하는 mel spectrogram을 데이터 형태로 바꾼다면?  
`(batch_size, mel_len, n_mels)`이다.

즉 Decoder Stack을 통과한 데이터에 `(hidden_dim, n_mels)`의 사이즈를 가지는 `Linear`를 통과시키면 해당 데이터를 mel spectrogram의 예측값으로 사용할 수 있다.

그리고 이 `Linear`가 [아키텍쳐](#architecture)의 마지막 부분에 있는 `Mel Linear`이다.

### \<SOS\> \<EOS\> \<PAD\> Token Of Mel Spectrogram

사실 그럼에도 풀리지 않는 문제가 하나 남아있다. token을 사용하지 않기 때문에 발생하는 의문점.

**\<SOS\> 토큰과 \<EOS\> 토큰이 없으면 예측의 시작과 마무리를 어떻게하지..?**  
**\<PAD\> 토큰이 없으면 각기 다른 sequence length를 어떻게 맞추지..?**

우선 \<PAD\> 토큰은 간단하다. 그냥 해당 부분의 mel spectrogram을 전부 0으로 채우면 된다.  
직관적으로 생각해봐도 mel spectrogram의 각 칸은 해당 시간대의 해당 주파수부분의 음량이라고 보면 된다. 이게 0이면 소리가 나지 않는다.

남은 것은 \<SOS\> 토큰과 \<EOS\> 토큰이다.  
[챗봇 만들기](https://fidabspd.github.io/2022/03/02/transformer_chatbot-3.html#qna)의 Transformer의 예측 수행 과정을 되새겨보면 다음과 같은 과정을 따른다.

1. input을 Transformer의 Encoder에 넣고 self attention의 output을 계산한다.
2. Encoder의 self attention의 output과 \<SOS\>토큰을 Decoder의 input으로 넣고 \<SOS\>토큰의 다음 token으로 나올 단어들의 확률을 얻는다.
3. \<SOS\>토큰의 다음 token으로 나올 단어들 중 확률이 가장 높은 단어를 \<SOS\>토큰 다음 sequence로 추가하고, 이전에 계산한 Encoder의 self attention의 output과 함께 Decoder에 input으로 넣는다.
4. 3.을 반복하여 다음 token으로 나올 단어들 중 \<EOS\>의 확률이 가장 높을 때 예측을 멈춘다.

이를 현재 상황에 대입해보면, \<SOS\>토큰과 \<EOS\>토큰을 어떻게 사용해야할지 모르기 때문에 예측의 시작으로 들어갈 값과 언제 예측을 멈춰야할지를 결정하지 못했다.

사실 \<SOS\>토큰도 간단하다. sequence의 길이가 1인 mel spectrogram을 0으로 채워 decoder의 첫 input으로 넣으면 된다. 즉 \<PAD\>와 같다. 그래도 상관 없는 것이 둘을 굳이 구분할 필요는 없다.

문제는 \<EOS\> 토큰이다. 이는 당연하게도 모두 0으로 채운다고 해결되지 않는다. 그냥 말을 중간에 끊은 것이라면? 

### Mel Spectrogram To WAV (Vocoder)

이제 mel spectrogram을 어떻게 만들고 이를 어떻게 예측할지 정도는 알았다. 그런데 결국 뽑아내고싶은 결과물은 오디오파일. 즉 wav파일이다.  
mel spectrogram을 wav파일로 복원하려면 어떻게 해야할까?

mel spectrogram(혹은 그냥 spectrogram)을 audio(현재는 wav)로 복원하는 것을 `Vocoder`라고 부른다.  
우선 결론부터 말하자면 Vocoder를 통한 완벽한 복원은 불가능하다. 이는 당연한 것이 wav를 mel spectrogram으로 변환하는 과정에서 정보의 손실이 발생했기 때문이다.

Vocoder 구현 방법은 크게 두가지가 있다.

1. wav를 mel spectrogram으로 변환한 것을 역 연산 하듯 하는 rule based 방법
2. 딥러닝을 사용하여 복원하는 방법

1번의 방법은 대표적으로 `griffin lim`이라는 방법이 있고 librosa의 기본 기능을 사용하면 되기 때문에 사용하기도 굉장히 편하다.  
2번의 방법은 당연히 별도의 훈련이 필요하며 현재 참고하고 있는 논문인 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)에서 `Post-net`이라는 이름으로 사용하고 있는 방법이기도 하다.

mel spectrogram을 예측하는 모델을 완성하고 나면, 우선은 예측 결과물을 griffin lim을 통해 wav로 복원해보고 영 시원치 않으면 2번 방법을 사용해보도록 하자.

