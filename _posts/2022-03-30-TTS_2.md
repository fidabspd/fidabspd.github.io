---
title: TTS - Audio Preprocessing & Dataset
excerpt: ""
tags: [TTS, Transformer, PyTorch]
toc: true
toc_sticky : true
use_math: true
excerpt: ""
---

지난 포스팅에서 텍스트 전처리와 짝이 Text-Speech를 다시 맞춰주는 Data Cleansing을 진행했다.  
이에 이어 Audio관련 처리와 TTS 훈련을 위한 데이터셋을 완성해보자.

# 의문점

지난 포스팅에서 들었던 의문점을 다시 돌이켜보자.

- Text는 단어 임베딩을 하는 것이 맞는 것일까
- Text의 숫자, 기호, 영어 등은 어떻게 처리해야할까
- Speech의 Sequence 길이는 감당할 수 있는 길이일까
- Speech를 정수로 Tokenizing을 할 수 있을까

이중 처음 두개는 해결했다.  
그래도 여전히 TTS를 구현하기에는 의문점이 남는다. 남은 의문점들에 대해 생각해보자.

# Audio

`librosa`를 통해 오디오를 불러와보자.

```python
import librosa
import matplotlib.pyplot as plt

fpath = '../data/wav/여1_동화1/1.wav'

origin, _ = librosa.load(fpath, sr=22050)
print(f'wav file shape: {origin.shape}')
print(origin)

plt.plot(origin)
plt.show()
```

![wav_shape_and_plot](/images/for_post/TTS_2/wav_shape_and_plot.png)

음성 파일은 이와같이 불러올 수 있다.  
저번 포스팅에서 봤듯이 이 파일은 '다람쥐와 호랑이'이다.  
여기서 뭔지 모르겠는 `sr`이라는 parameter가 하나 있다.

## Sample Rate

`sr`은 Sample Rate의 약자이다.  
원래는 아날로그 신호인 소리라는 공기의 진동을 디지털 신호로 전환하기 위해 초당 몇번의 신호를 수집했는지를 나타내는 parameter이다.  
오디오 파일이 만들어질 당시부터 정해지는 값이기 때문에 오디오 파일 자체에서 확인을 해야한다.

![wav_file_properties](/images/for_post/TTS_2/wav_file_properties.png)

오디오파일의 정보이다. `Sample rate`부분에 `44100Hz`로 표시되어있는 것을 볼 수 있는데 이를 2로 나누어 사용하면 된다.  
2로 나누는 이유에 대해 찾아보면 진동이라는건 기본적으로 위로한번 아래로 한번 어쩌고..하는 등의 설명을 찾아볼 수 있는데 결론은 `librosa`가 그렇게 만들어져있는 것이다.

아무튼 이 `sr`은 높을수록 음질이 좋다고 생각하면 되며 표현할 수 있는 음역의 폭이 넓어진다. 그리고 대부분의 오디오파일은 44100Hz의 Sample Rate를 가지며 librosa의 디폴트값도 22050이다.

불러온 wav파일의 모양은 1차원의 길이가 53358인 데이터이다. 이를 앞에서 설명한 sr과 연결해 생각해보면  
1초에 데이터를 22050번 수집한 것이므로 길이가 53358인 데이터는 2초가 조금 넘는 길이인 것이다.

***2초짜리 데이터의 sequence 길이가 53358..?*** 이대로는 안된다는 것을 딱 봐도 알 수 있다. 이걸 예측하려면 sample rate 22050 기준 1초의 오디오를 예측하려면 22050번의 반복문을 돌아야한다.  

Sample rate를 줄이면 어느정도 길이가 줄어들겠지만 음질을 망칠뿐더러 음역폭도 너무 줄어드는 탓에 한계가 있다.  
그럼 어떻게 해야할까.

## Mel Spectrogram

방법은 바로 Mel Spectrogram을 이용하면 된다.

Mel Spectrogram에 대해서 완벽하게 이해하려면 푸리에 변환과 음성 신호에 대한 깊은 이해가 필요하다.  
이에 대해서는 깊게 다루지는 않을 예정이며 변환하기 위해서는 어떤 코드를 사용하며, 변환 결과가 어떤 의미를 가지는지 정도만 알아보자.

우선 스펙트로그램이란 무엇인지 간단히 알아보자.

>스펙트로그램(Spectrogram)은 소리나 파동을 시각화하여 파악하기 위한 도구로, 파형(waveform)과 스펙트럼(spectrum)의 특징이 조합되어 있다.  
>파형에서는 시간축의 변화에 따른 진폭 축의 변화를 볼 수 있고, 스펙트럼에서는 주파수 축의 변화에 따른 진폭 축의 변화를 볼 수 있는 반면, 스펙트로그램에서는 시간축과 주파수 축의 변화에 따라 진폭의 차이를 인쇄 농도 / 표시 색상의 차이로 나타낸다.  
[출처 wikipedia](https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%8E%99%ED%8A%B8%EB%A1%9C%EA%B7%B8%EB%9E%A8)

쉽게 말하면 X축은 시간(Time), Y축은 주파수(Hz), 색깔은 소리의 크기(dB)라는 얘기다.

그리고 이중에서도 Mel Spectrogram이란 Spectrogram에 Mel-filter를 적용해서 얻은 결과물로 사람의 청각기관이 고주파보다는 저주파에 민감한 특징을 이용하여 사람이 들을 수 있는 정보만을 골라낸 것이라고 생각하면 편하다. (자세한 설명은 여기 [현토리님의 블로그](https://hyunlee103.tistory.com/46?category=999732)를 참고하도록 하자.)

앞서 보여준 오디오를 스펙트로그램으로 바꿔서 시각화한 결과를 보자.

![spectrogram](/images/for_post/TTS_2/spectrogram.png)

그림을 보면 직관적으로 이해 가능하다.

### How To Convert

이제 원래의 `wav`파일을 `mel spectrogram`으로 바꾸는 방법에 대해 알아보자.  
코드부터 보자면 다음과 같다.

```python
def load_wav(fpath, sr):
    wav, _ = librosa.load(fpath, sr=sr)
    return wav

def get_mel(fpath, sr, n_mels, n_fft, win_length, hop_length):

    y = load_wav(fpath, sr=sr)
    
    mel = librosa.feature.melspectrogram(
        y=y, n_mels=n_mels, n_fft=n_fft,
        hop_length=hop_length, win_length=win_length)
    
    return mel.T


wav = load_wav(audio_file_path, 22050)
mel = get_mel(audio_file_path, 22050, 80, 2048, int(22050*0.05), int(22050*0.0125))

print(f'wav.shape: {wav.shape}')
print(f'mel.shape: {mel.shape}')  # (seq_len, n_mels)
```

```
wav.shape: (53358,)
mel.shape: (195, 80)
```

위와 같이 wav파일의 mel spectrogram을 얻을 수 있다.  
(최종 return 값에 transpose를 해준 이유는 `(seq_len, n_mels)`의 shape으로 만들기 위해서이다.)

그런데 parameter 설정이 보인다. 이에대해 알아보자  
(다음 설명들은 모두 쉽게 이해하기 위한 설명으로 완벽한 설명은 아니다. 또한 이 외에도 다른 parameter들이 존재하지만 사용하는 것만 설명하도록 한다.)

- `n_mels`: 사람이 듣는 주파수를 몇단계로 분리할 것인지를 결정한다. 다른말로 설명하자면 위 mel spectrogram 시각화의 row 개수라고 생각하면 된다.
- `n_fft`: time 도메인의 정보를 frequency 도메인으로 변환할때 몇개의 구간으로 나눌지를 결정한다. `win_length`보다 큰 값을 가지면 smoothing의 효과가 있다.
- `win_length`: `wav`파일의 몇칸을 `mel spectrogram`에서 한칸으로 만들것인가를 결정한다. 다만 인간이 인지하기 쉬운 time의 영역이 아니라 디지털 신호의 개수를 의미하므로 보통 time영역에서 `frame_length`를 결정하고 `win_length = int(sr*frame_length)`로 사용하는 경우가 많다.
- `hop_length`: `wav`파일에서 몇칸씩 띄어가며 window를 만들지 결정한다. 보통 `win_length`보다 짧게 설정하며, `win_length`보다 `hop_length`가 짧으면 smoothing의 효과가 있다. `win_length`와 마찬가지로 디지털 신호 자체의 영역에서 정의하기보단, time영역에서 `frame_shift`를 결정하고 `hop_length = int(sr*frame_shift)`로 사용하는 경우가 많다.

변환 결과를 보면 sequence의 길이가 53358에서 195로 짧아졌다. 엄청난 효과이다.

이렇게 세번째 의문점이었던 ***Speech의 Sequence 길이는 감당할 수 있는 길이일까***에 대해 해결했다.

하지만 여전히 의문점은 남아있다.  
이걸 ***이걸 정수 Tokenizing 할 수 있을까?***

## How To Train

원래의 Transformer 아키텍쳐에 따르면 디코더는 Target의 모든 토큰 중 다음 토큰으로 등장할 확률이 가장 높은 토큰 하나를 softmax를 통해 예측하고 해당 예측값을 원래의 sequence와 loss를 계산하게 되어있다.  
이 방법을 사용하기 위해서는 정수 Tokenizing이 필수이다.

그런데 굳이 이 방법을 사용해야할까?

다시한번 아키텍쳐를 살펴보자.

![architecture_comparison](/images/for_post/TTS_2/architecture_comparison.png)

우선 기본적인 Transformer의 아키텍쳐에서 디코더의 마지막에 붙어있는 `Linear`에 들어가기 전 데이터의 shape에 대해 생각해보자.

이때 데이터의 shape은 `(batch_size, target_len(query_len), hidden_dim(d_model))`이다.  
여기에 `(hidden_dim, n_mels)` shape의 `Linear`를 덧붙인다면?  
이 레이어의 output shape은 `(batch_size, speech_len(mel_len), n_mels)`가 된다.

그리고 이를 최종 output값으로 사용하면 mel spectrogram 자체를 예측값으로 사용할 수 있다.  
그러니까 정수 Tokenizing 안해도 된다.

그리고 `(hidden_dim, n_mels)` shape의 `Linear`가 바로 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)의 아키텍쳐에 나오는 `Mel Linear`이다. (정확히는 조금 다르지만 자세한건 모델 부분에서 설명 예정.)

이제 훈련은 할 수 있게 되었다.  
그래도 정수 Tokenizing이 없기 때문에 이해가 되지 않는 부분이 있다.  

***\<SOS\> 토큰과 \<EOS\> 토큰이 없으면 예측의 시작과 마무리를 어떻게하지..?***  
***\<PAD\> 토큰이 없으면 각기 다른 sequence length를 어떻게 맞추지..?***

## How To Predict

### \<PAD\>

Tensor는 기본적으로 각 벡터의 차원이 동일해야 모델의 input으로 들어갈 수 있다. 따라서 sequence의 length를 맞춰주는 것은 기본이다. 이는 padding을 통해 진행해야할텐데 정수 Token이 없는 현 상황에서 padding을 어떻게 수행해야할까.

사실 Token의 형태라고 보기는 애매하지만 편의상 \<PAD\> 토큰이라고 부르도록 하자.  
우선 \<PAD\> 토큰은 간단하다. 그냥 해당 부분의 mel spectrogram을 전부 0으로 채우면 된다.  
직관적으로 생각해봐도 mel spectrogram의 각 칸은 해당 시간대의 해당 주파수부분의 음량이라고 보면 된다. 이게 0이면 소리가 나지 않는다.

### How To Stop Predicting

남은 것은 \<SOS\> 토큰과 \<EOS\> 토큰이다.  
[챗봇 만들기](https://fidabspd.github.io/2022/03/02/transformer_chatbot-3.html#qna)의 Transformer의 예측 수행 과정을 되새겨보면 다음과 같은 과정을 따른다.

1. input을 Transformer의 Encoder에 넣고 self attention의 output을 계산한다.
2. Encoder의 self attention의 output과 \<SOS\>토큰을 Decoder의 input으로 넣고 \<SOS\>토큰의 다음 token으로 나올 단어들의 확률을 얻는다.
3. \<SOS\>토큰의 다음 token으로 나올 단어들 중 확률이 가장 높은 단어를 \<SOS\>토큰 다음 sequence로 추가하고, 이전에 계산한 Encoder의 self attention의 output과 함께 Decoder에 input으로 넣는다.
4. 3.을 반복하여 다음 token으로 나올 단어들 중 \<EOS\>의 확률이 가장 높을 때 예측을 멈춘다.

이를 현재 상황에 대입해보면, \<SOS\>토큰과 \<EOS\>토큰을 어떻게 사용해야할지 모르기 때문에 예측의 시작으로 들어갈 값과 언제 예측을 멈춰야할지를 결정하지 못했다.

### \<SOS\>

\<SOS\>토큰도 간단하다.  
sequence의 길이가 1인 mel spectrogram을 0으로 채워 decoder의 첫 input으로 넣으면 된다. 즉 \<PAD\>와 같다.  
그래도 상관 없는 것이 둘을 굳이 구분할 필요는 없다.

### \<EOS\>

문제는 \<EOS\> 토큰이다. 이는 당연하게도 모두 0으로 채운다고 해결되지 않는다.  
만약 이렇게 한다면 그냥 말을 중간에 끊은 것도 예측이 끝난 것이라고 보게 될 것이다.  
그리고 중간에 말이 끊기지 않고 이어진다한들 문장이 끝났다고 모델의 output이 순수하게 모든 것을 0으로 뱉을 가능성도 매우 낮다(사실상 없다).

그럼 어떻게 앞선 Transformer의 예측 수행 과정 중 4번과 같이 예측을 멈출 수 있을까?

두가지 방법이 있다.

1. Stop해야하는지 여부 자체를 예측하는 모델을 따로 만든다.
2. 그냥 Stop하지 않는다.

우선 첫번째 방법은 오디오파일의 padding을 시작하기 전 마지막부분에 멈추면 됨 자체를 학습하는 것이다.  
현재 참고중인 논문인 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)에서 제안하는 방법이기도 하다.  
하지만 직관적으로 생각해봐도 이는 쉽지 않다. 해당 seq까지의 정보만가지고 멈출지를 정해야하는데 이에 대한 학습 데이터는 굉장히 imbalanced할 것이 뻔하다. 왜냐하면 멈추는 부분은 전체 seq중에 딱 한 곳 뿐이기 때문이다. seq길이가 1000이라면 멈추는 데이터 한개, 계속 진행하는 데이터 999개이다.  
그리고 논문에서도 이에 대한 지적이 있다.

두번째 방법은 사실 예측을 멈추는 방법은 아니긴 하지만 고민 자체를 해결할 수 있기는 하다. **굳이 멈춰야하나..?**를 생각해보면 된다.  
정상적으로 학습이 이루어졌다면 문장이 끝난시점부터는 speech를 생성하지 않는 것이 정상이다. 그러니 예측을 계속한다한들 빈 오디오가 나올 것이기 때문에 멈추지 않는 것도 방법이 된다.

우선은 두번째 방법을 사용하여 speech를 생성해보고 원하는 결과가 나오지 않으면 별도의 모델을 만드는 것을 고려해보도록 하자.

그럼 mel spectrogram을 예측 할 수는 있게 되었다.  
근데 결국 만들고싶은건 text를 speech로 바꿔주는 것인데, 그러려면 mel spectrogram을 다시 1d array 형태로 바꾸고 오디오파일로 만드는 과정이 필요하다.

이는 어떻게 할 수 있을까.

## Mel Spectrogram To Audio (Vocoder)

결론부터 말하자면 mel spectrogram을 원래의 audio로 완벽하게 복원하는 것은 불가능하다. 이는 당연한 것이 wav를 mel spectrogram으로 변환하는 과정에서 정보의 손실이 발생했기 때문이다.

그래도 복원하는 방법이 있다. 이 복원하는 것을 `Vocoder`라고 한다.
Vocoder 구현에는 크게 두가지 방법이 있다.

1. **wav를 mel spectrogram으로 변환한 것을 역 연산 하듯 하는 rule based 방법**  
대표적으로 `griffin lim`이라는 방법이 있고 librosa의 기본 기능을 사용하면 되기 때문에 사용하기도 굉장히 편하다.
2. **딥러닝을 사용하여 복원하는 방법**  
이 방법은 당연히 별도의 훈련이 필요하며 현재 참고하고 있는 논문인 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)에서 `Post-net`이라는 이름으로 사용하고 있는 방법이기도 하다.

1번 방법인 Griffin Lim을 테스트하기 위해 '다람쥐와 호랑이'를 mel spectrogram으로 변환했다가 다시 되돌리고 그 결과를 원본 오디오와 비교해보자.  

```python
import librosa
import soundfile as sf

SR = 22050
N_MELS = 80
N_FFT = 2048
FRAME_SHIFT = 0.0125
FRAME_LENGTH = 0.05
HOP_LENGTH = int(SR*FRAME_SHIFT)
WIN_LENGTH = int(SR*FRAME_LENGTH)

fpath = '../data/wav/여1_동화1/1.wav'

origin, _ = librosa.load(fpath, sr=SR)
mel = librosa.feature.melspectrogram(
    y=origin, n_mels=N_MELS, n_fft=N_FFT,
    hop_length=HOP_LENGTH, win_length=WIN_LENGTH)
mel = mel.T

inversed = librosa.feature.inverse.mel_to_audio(
    mel.T, sr=SR, hop_length=HOP_LENGTH, win_length=WIN_LENGTH)

sf.write('sample_audio_inversed.wav', inversed, SR)
```

**Original Audio:**  
<audio controls>   
    <source src="/images/for_post/TTS_2/sample_audio.wav" type="audio/wav">   
    Your browser does not support the audio element.   
</audio>

**Mel Spectrogram ➞ Audio Inversed:**  
<audio controls>   
    <source src="/images/for_post/TTS_2/sample_audio_inversed.wav" type="audio/wav">   
    Your browser does not support the audio element.   
</audio>

비교결과 Griffin Lim을 통해 복원한 오디오가 원본 오디오에 비해 훨씬 음질이 떨어지고 잡음이 많이 섞여있음을 알 수 있다.

우선은 mel spectrogram을 예측하는 모델을 완성하는 것을 목표로하고 이에 대한 복원은 griffin lim을 통해 진행한다.  
그 결과가 영 시원치 않으면 그때 2번 방법을 사용하여 별도의 모델을 만들도록 하자.

# Dataset

이제 어떻게 훈련하고 예측하면 될지 정확하게 알았다.  
단일 화자 TTS를 위한 Dataset을 만들어보자.

## Vocabulary Dictionary

```python
PAD = '_'
SOS = '@'
EOS = '|'
PUNC = ['.', ',', '?', '!', '\'', '\"', '-', '~', '…']
SPACE = ' '

JAMO_LEADS = [chr(_) for _ in range(0x1100, 0x1113)]
JAMO_VOWELS = [chr(_) for _ in range(0x1161, 0x1176)]
JAMO_TAILS = [chr(_) for _ in range(0x11A8, 0x11C3)]

VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + PUNC + [SPACE]
ALL_SYMBOLS = [PAD] + [SOS] + [EOS] + VALID_CHARS

char_to_id = {c: i for i, c in enumerate(ALL_SYMBOLS)}
id_to_char = {i: c for i, c in enumerate(ALL_SYMBOLS)}
```

## Load Scripts & Tokenize

```python
def get_data_list(speaker, wav_path):
    pattern = re.compile(f'^{speaker}_')
    data_list = [folder for folder in os.listdir(wav_path) if pattern.match(folder) and '어학' not in folder]
    return data_list

def load_script(file_path, sheet_name):
    return pd.read_excel(file_path, sheet_name=sheet_name)


def tokenize(text, as_id=False):
    tokens = list(hangul_to_jamo(text))

    if as_id:
        return [char_to_id[SOS]] + [char_to_id[token] for token in tokens] + [char_to_id[EOS]]
    else:
        return [SOS] + [token for token in tokens] + [EOS]
```

## Normalize Text

```python
def normalize_text(text):
    text = normalize_etc(text)
    text = normalize_quote(text)
    text = normalize_en(text)
    text = normalize_num(text)
    return text
```

이전에 만들었던 `normalize_text` 그대로이다.

## Load Audio & Make Mel Spectrogram

```python
def get_mel(fpath, sr, n_mels, n_fft, hop_length, win_length):

    y, _ = librosa.load(fpath, sr=sr)
    y, _ = librosa.effects.trim(y)
    
    mel = librosa.feature.melspectrogram(
        y=y, n_mels=n_mels, n_fft=n_fft,
        hop_length=hop_length, win_length=win_length)
    
    return mel.T.astype(np.float32)
```

추가로 앞 뒤 공백 오디오를 잘라주는 `trim`을 추가했다.  

## Make Dataset

### Dataset For One Category

```python
class TextMelDataset(Dataset):
    
    def __init__(self, script_path, sheet_name, audio_path, sr, n_mels, n_fft,
                 hop_length, win_length):
        self.scripts = load_script(script_path, sheet_name)
        self.audio_path = audio_path
        self.sr = sr
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.win_length = win_length

        self.audio_file_list = os.listdir(self.audio_path)
    
    def __len__(self):
        return len(self.audio_file_list)
    
    def __getitem__(self, idx):
        file_name = self.audio_file_list[idx]
        file_num = int(file_name[:-4])

        # script
        script = self.scripts.query(f'Index == {file_num}')['대사'].tolist()[0]
        script = normalize_text(script)
        tokens = tokenize(script, as_id=True)
        
        # audio
        fpath = os.path.join(self.audio_path, str(file_num)+'.wav')
        mel = get_mel(fpath, self.sr, self.n_mels, self.n_fft, self.hop_length, self.win_length)
        mel = np.concatenate([
                np.zeros([1, self.n_mels], np.float32),
                mel,
                np.zeros([2, self.n_mels], np.float32)
        ], axis=0)  # <sos> + mel + <eos>
        
        return {'text': tokens, 'speech': mel, 'text_len': len(tokens), 'speech_len': len(mel)}
```

하나의 스크립트를 로드하고 그에 맞는 오디오를 불러오는 데이터 셋이다.

`mel`에 \<SOS\> 토큰과 \<EOS\>를 추가해줬다.

### Concatenate Dataset

```python
def get_single_speaker_dataset(speaker, wav_path, script_path,
                               sr, n_mels, n_fft, hop_length, win_length):

    data_list = get_data_list(speaker, wav_path)

    concat_dataset = []
    print(f'Loading {data_list} ...')
    for sheet_name in data_list:
        sheet_name = sheet_name.split('_')[1]
        
        audio_path = os.path.join(wav_path, speaker+'_'+sheet_name)
        
        text_mel_dataset = TextMelDataset(
            script_path, sheet_name, audio_path, sr, n_mels, n_fft,
            hop_length, win_length)
        concat_dataset.append(text_mel_dataset)
        print(f'{sheet_name} Done!')
        
    return ConcatDataset(concat_dataset)


ds = get_single_speaker_dataset(
    SPEAKER, WAV_PATH, SCRIPT_FILE_NAME, SR, N_MELS, N_FFT, HOP_LENGTH, WIN_LENGTH)
```

`TextMelDataset`은 하나의 스크립트에 대한 데이터만 불러오기 때문에 한명의 화자에 대한 모든 데이터를 포함한 Dataset을 만들기 위해 `ConcatDataset`을 진행한다.

### Data Loader

```python
def pad_tokens(tokens, max_len):
    len_tokens = len(tokens)
    return np.pad(tokens, (0, max_len-len_tokens))


def pad_mel(mel, max_len):
    len_mel = len(mel)
    return np.pad(mel, ((0, max_len-len_mel), (0, 0)))


def collate_fn(batch):

    max_text_len = max([data['text_len'] for data in batch])
    max_speech_len = max([data['speech_len'] for data in batch])

    text = np.stack([pad_tokens(data['text'], max_text_len) for data in batch])
    speech = np.stack([pad_mel(data['speech'], max_speech_len) for data in batch])

    return torch.LongTensor(text), torch.FloatTensor(speech)


dl = DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)
```

`DataLoader`에 `collate_fn`을 추가했다.

이를 추가한 이유는 padding을 통해 sequence length를 맞추기 위함이다.  
`Dataset`에서 해도 되는데 굳이 `collate_fn`까지 써가면서 `DataLoader`에서 padding을 진행한 것은 그에 장점이 있기 때문이다.

그 장점은 batch안에서 만 sequence length를 맞추고 padding을 진행하면 된다는 것이다. 그러면 모든 데이터의 길이를 max sequence length를 맞추지 않아도 된다.  
그냥 하나의 batch 안에서 가장 긴 sequence 길이를 가지는 데이터에 맞춰 padding을 진행하면 된다. 이렇게 메모리를 조금이나마 아낄 수 있다.

이게 가능한 이유는 transformer의 구조 자체에 있다.  
transformer의 그 어떤 부분도 input으로 들어오는 데이터의 sequence길이에 dependent하지 않다. 대부분의 벡터를 hidden_dim에 맞추기 때문에 가능하다.  
딱 한가지. positional embedding에서 정의한 input dimension보다 긴 길이의 데이터가 들어오지만 않으면 된다.

## 마무리

`DataLoader`까지 완성했다. 이제 진짜 모델만 만들면 바로 훈련할 수 있다.

다음 포스팅에서는 아키텍쳐에 맞게 모델을 만들어보자.
