---
title: Time Series - Multi-Task Learning 시계열 모델 (1)
tags: [Time-Series, Tensorflow, Keras]
---

앞선 모든 시계열 시리즈에서는 `num == 1`인 데이터만 사용하였다. 하지만 주어진 데이터는 60개의 건물 모두에 대한 sequence가 데이터이다. 이 모두를 이용하여 데이터와 모델을 구성해보자.

## 목차

1. [시계열 데이터의 기본적인 특징과 간단한 모델](https://fidabspd.github.io/2022/01/04/time_series_1.html)
1. [tf.data.Dataset을 이용한 시계열 데이터 구성](https://fidabspd.github.io/2022/01/10/time_series_2.html)
1. [Multi-Input 시계열 모델](https://fidabspd.github.io/2022/01/18/time_series_3.html)
1. [**Multi-Task Learning 시계열 모델 (1)**](https://fidabspd.github.io/2022/01/28/time_series_4-1.html)
1. [Multi-Task Learning 시계열 모델 (2)](https://fidabspd.github.io/2022/01/30/time_series_4-2.html)
1. [시계열 target의 결측](https://fidabspd.github.io/2022/02/10/time_series_5.html)
1. [Recursive Input Prediction](https://fidabspd.github.io/2022/02/17/time_series_6.html)

## 원본 코드 ➞ [<span style="color:#AC1538">CODE (GitHub)</span>](https://github.com/fidabspd/time_series/tree/master/codes/4_multi_task_learning.ipynb)

***

## 시작하기 전에

- **본 포스팅 시리즈의 목적은 시계열 예측의 정확성을 높이기 위함이 아니다.**
- **단지 시계열 데이터를 다루고 예측함에 있어 통상적으로 부딪히는 여러 문제들과 그를 어떻게 해결하면 좋을 지 가이드를 제시하는 것에 목적이 있다.**
- **데이터는 [DACON 전력사용량 예측 AI 경진대회](https://dacon.io/competitions/official/235736/overview/description)의 데이터를 사용한다.** 감사합니다 DACON!
    - 60개의 건물의 전력 수요량을 미리 예측하는 것이 목적이며, 본 대회는 그를 위한 데이터를 다음과 같이 제공한다.
    - 데이터 column 구성
        - `num`: 건물 번호
        - `date_time`: 데이터가 측정된 시간 (1시간 단위)
        - `target`: 전력 사용량 (예측해야하는 값)
        - `temp`: 기온(°C)
        - `wind`: 풍속(m/s)
        - `humid`: 습도(%)
        - `rain`: 강수량(mm)
        - `sun`: 일조(hr)
        - `non_elec_eq`: 건물별 비전기 냉방 설비 운영 여부 (1: 보유, 0: 미보유)
        - `sunlight_eq`: 건물별 태양광 설비 보유 여부 (1: 보유, 0: 미보유)
    - **이번 포스팅에서는 모든 데이터를 사용**

***

## Multi-Task Learning

**Multi-Task Learning**: Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. [출처: wikipedia](https://en.wikipedia.org/wiki/Multi-task_learning)

위키피디아의 설명과 같이 Multi-Task Learning이란 관계가 있는 여러 학습을 동시에 수행하며, 개별적으로 훈련할때 보다 높은 효율성과 정확성을 기대할 수 있다.

즉 현재 사용하고 있는 데이터에 적용하면 '60개의 건물을 개별적으로 훈련하지 않고 동시에 수행함으로써 보다 높은 효율성과 정확성을 기대한다.'로 바꿔 말할 수 있다. 그리고 현재 작성중인 시계열 시리즈는 정확성보다는 케이스 탐구에 집중하고 있기 때문에 효율성(보다 빠른 훈련)에 초점을 두도록 하겠다.  
만약 multi-task learning을 활용하지 않는다면 어떻게 해야할까? 당연히 개별적으로 훈련을 해야한다. 즉 모델을 60개 만들어야한다는 결론에 이르게 된다. 사실 현재의 경우에는 데이터가 그렇게 크지는 않기 때문에 60개의 모델을 따로 만든다해도 크게 문제가 되진 않을 것이다. 하지만 60개가 2000개가 된다면? 각 sequence의 길이가 3년어치라면? 문제는 심각해진다.  

정확도의 측면에서 본다면 사실 모델을 따로 만드는 것이 더 정확할 수도 있다.  
multi-task learning은 각 task가 얼마나 깊은 관계가 있는지 등에 따라 크게 영향을 받는다. 극단적인 예시를 들어보자면 삼성전자의 주가와 불닭볶음면의 판매량을 예측하는데 이 둘을 multi-task learning으로 묶어 같은 feature로 예측한다면 잘 작동할 리가 없다.  
실제 업무에서 이를 고려중이라면 데이터의 분포를 보고 관련있는 task들을 묶고 모델을 몇개를 만들지, Deadline, Hardware resource, 최소한의 성능 등을 고려하여 multi-task learning을 진행해야한다.  

하지만 현재 포스팅에서는 모든 건물을 한번에 묶어 multi-task learning을 진행하기로 한다.

## CODE

### Libraries

```python
from copy import deepcopy

import numpy as np
import pandas as pd

import datetime

import tensorflow as tf
from tensorflow.data import Dataset
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Loss
from tensorflow.keras.metrics import Metric
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
```

이번 포스팅에서는 custom loss와 custom metric을 작성하며 간단하게 tensorboard log 또한 남긴다.

### Set Configs

```python
CONFIGS = {
    'data_path': '../data/',
    'model_path': '../model/',
    'model_name': 'multi_task_learning',
    'model_type': 'cnn1d',
    
    'dtype': tf.float32,
    
    'valid_start_date_time': '2020-08-11 00',
    'test_start_date_time': '2020-08-18 00',
    
    'batch_size': 64,
    'learning_rate': 5e-5,
    'epochs': 100,
    'es_patience': 10,
    
    'window_size': 7*24,
    'shift': 1,
    'target_length': 3,
}

CONFIGS['tensorboard_log_path'] = f'../logs/tensorboard/{CONFIGS["model_name"]}'
```

### Load Data

```python
train_origin = pd.read_csv(CONFIGS['data_path']+'train.csv', encoding='cp949')

data = deepcopy(train_origin)

data.columns = [
    'num', 'date_time', 'target', 'temp', 'wind',
    'humid', 'rain', 'sun', 'non_elec_eq', 'sunlight_eq'
]

data['num'] -= 1

CONFIGS['n_buildings'] = len(data['num'].unique())

data
```

![data](/post_images/time_series_4-1/data.png)

- 각 건물들의 index인 `num`이 1부터 시작하기 때문에 1을 빼준다.  
- 건물의 총 개수를 저장해둔다.

### 시간 데이터 가공

```python
def mk_time_data(data):
    
    new_data = data.copy()

    new_data['date_time'] = data['date_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H'))
    
    new_data['time_stamp'] = new_data['date_time'].apply(lambda x: x.timestamp())
    
    new_data['year'] = new_data['date_time'].apply(lambda x: x.year)
    new_data['month'] = new_data['date_time'].apply(lambda x: x.month)
    new_data['day'] = new_data['date_time'].apply(lambda x: x.day)
    
    new_data['hour'] = new_data['date_time'].apply(lambda x: x.hour)
    new_data['cos_hour'] = np.cos(2*np.pi*(new_data['hour']/24))
    new_data['sin_hour'] = np.sin(2*np.pi*(new_data['hour']/24))

    new_data['weekday'] = new_data['date_time'].apply(lambda x: x.weekday())
    new_data['cos_weekday'] = np.cos(2*np.pi*(new_data['weekday']/7))
    new_data['sin_weekday'] = np.sin(2*np.pi*(new_data['weekday']/7))
    
    new_data['is_holiday'] = 0
    new_data.loc[(new_data['weekday'] == 5) | (new_data['weekday'] == 6), 'is_holiday'] = 1
    new_data.loc[(new_data['month'] == 8) & (new_data['day'] == 17), 'is_holiday'] = 1
    
    return new_data


new_data = mk_time_data(data)
```

- `str`을 `datetime`으로 형변환 한다.
- `datetime`을 `int`로 변환하여 `timestamp`를 만든다.
- `datetime`에서 연, 월, 일, 요일 정보를 따로 뺀다.
- 순환하는 변수는 cyclical features encoding을 진행해준다. ([cyclical features encoding 참고](https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca))
- 공휴일 정보 등 추가로 활용하고 싶은 정보를 추가한다.

### Make Building Info

```python
def mk_building_info(data, data_for_calc, CONFIGS):
        
    new_data = data.copy()
    new_data['range'] = 0
    new_data['mean'] = 0
    new_data['std'] = 0
    new_data['holiday_gap'] = 0
    new_data['day_gap'] = 0

    for num in range(CONFIGS['n_buildings']):
        building = data_for_calc.query(f'num == {num}')
        
        bt_range = building['target'].max()-building['target'].min()
        bt_mean = building['target'].mean()
        bt_std = building['target'].std()
        bt_holiday_gap = abs(building.query('is_holiday == 0')['target'].mean() - building.query('is_holiday == 1')['target'].mean())
        bt_day_gap = 0
        for d in range(building.shape[0]//24):
            tmp = building['target'][d*24:(d+1)*24]
            bt_day_gap += (tmp.max()-tmp.min())/(building.shape[0]//24)
            
        new_data.loc[new_data['num']==num, 'range'] = bt_range
        new_data.loc[new_data['num']==num, 'mean'] = bt_mean
        new_data.loc[new_data['num']==num, 'std'] = bt_std
        new_data.loc[new_data['num']==num, 'holiday_gap'] = bt_holiday_gap
        new_data.loc[new_data['num']==num, 'day_gap'] = bt_day_gap
        
    new_data['mean_to_inverse'] = new_data['mean']
    new_data['std_to_inverse'] = new_data['std']
        
    return new_data


new_data = mk_building_info(
    new_data,
    new_data[new_data['date_time']<CONFIGS['valid_start_date_time']],
    CONFIGS
)

new_data
```

![new_data](/post_images/time_series_4-1/new_data.png)

이전과는 다르게 하나의 건물만을 사용하는 것이 아니라 60개의 건물을 함께 input으로 넣어주기 때문에 각 건물의 특징 값들을 새로 만들어준다.  
- `range`: 각 건물 `target`의 최대값-최소값
- `mean`: 각 건물 `target`의 평균
- `std`: 각 건물 `target`의 표준편차
- `holiday_gap`: 휴일과 휴일이 아닌날의 `target`평균값의 차
- `day_gap`: `target`값의 일별 최대값-최소값의 평균
- `mean_to_inverse`: 추후 scaling된 값을 다시 되돌리기 위해 잠시 추가 (뒤에서 추가 설명)
- `std_to_inverse`: 추후 scaling된 값을 다시 되돌리기 위해 잠시 추가 (뒤에서 추가 설명)

### Scaling

```python
def mk_mean_std_dict(data, scaling_by_building_cols):
    mean_std_dict = {}
    for num in range(60):
        building = data.query(f'num == {num}')
        mean_std_dict[num] = {
            col: {
                'mean': building[col].mean(),
                'std': building[col].std()
            } for col in scaling_by_building_cols
        }
    return mean_std_dict


scaling_by_building_cols = [
    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp', 'target',
]
scaling_by_all_cols = ['range', 'mean', 'std', 'holiday_gap', 'day_gap']

mean_std_dict = mk_mean_std_dict(
    new_data[new_data['date_time'] < CONFIGS['valid_start_date_time']],
    scaling_by_building_cols
)
CONFIGS['mean_std_dict'] = mean_std_dict


def standard_scaling(data, scaling_by_building_cols, scaling_by_all_cols, mean_std_dict=None):
    if not mean_std_dict:
        mean_std_dict = mk_mean_std_dict(data, scaling_by_building_cols)
        
    new_data = data.copy()
    for num in range(60):
        for col in scaling_by_building_cols:
            new_data.loc[new_data['num']==num, col] -= mean_std_dict[num][col]['mean']
            new_data.loc[new_data['num']==num, col] /= mean_std_dict[num][col]['std']
    
    for col in scaling_by_all_cols:
        m = new_data.loc[:, col].mean()
        s = new_data.loc[:, col].std()
        new_data.loc[:, col] -= m
        new_data.loc[:, col] /= s
    
    return new_data


new_data = standard_scaling(new_data, scaling_by_building_cols, scaling_by_all_cols, mean_std_dict)
```

Multi-task learning의 scaling은 일반적인 데이터의 scaling과 다르게 이루어진다. 각 task(현재는 건물)의 데이터의 특징이 다 다르기 때문이다.  
현재 모든 건물들의 `target`값은 천차만별이다. 어떤 건물은 `target`값이 10000근처에서 왔다갔다 하는 경우도 있는 반면에, 또 어떤건물은 `target`값의 평균이 1000정도인 건물도 있다. 따라서 시계열로 들어갈 feature들은 각 건물별로, 혹은 하나의 sequence마다 scaling을 해주는 것이 좋다.  

- 시계열 값들로 들어갈 `temp`, `target`등은 건물별로 standard scaling을 해준다.
- 각 건물의 고유정보인 `range`, `holiday_gap`등은 건물별이 아닌 데이터 통째로 standard scaling을 진행한다.
- 당연히 train에 사용할 기간의 데이터만 이용하여 mean과 std를 계산한다.

### Dataset

[지난 포스팅 multi-input](https://fidabspd.github.io/2022/01/18/time_series_3.html)에서 하나의 건물에 대해서 `tf.data.Dataset`을 이미 만들어보았다.  
하지만 지금은 60개의 건물에 대해서 만들어야하며 서로 다른 건물들은 하나의 sequence로 이어지면 안된다.  
이를 구현하는 방법은 두가지가 있다.

1. 데이터를 미리 task별로 자르고, 각각의 task에 대해서 dataset을 구성한다.
2. 데이터를 task별로 자르지 않고 통째로 stride를 이용한다.

2번 방법이 코드는 더 짧지만 직관적인 쪽을 꼽으라면 1번 방법인듯 하다. 2번 방법은 [Recursive Input Prediction](https://fidabspd.github.io/2022/02/17/time_series_6.html)에서도 다룰 예정이므로 지금은 1번 방법을 사용해보도록 하자.

거의 비슷한 형태로 진행되며 multi-input에서 데이터를 구성했던것을 똑같이 각각 60번 진행한다고 생각하면 편하다.  
다만 데이터를 먼저 건물별로 잘라내고 그 각각에 데이터 구성을 또 다시 적용하게 되므로 `map`, `flat_map`등을 많이 활용하므로 조금 복잡하다.

#### Input Columns

```python
building_num_cols = ['num']
building_info_cols = [
    'range', 'mean', 'std', 'holiday_gap', 'day_gap',
    'non_elec_eq', 'sunlight_eq',
]
target_time_info_cols = [
    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp',
    'cos_hour', 'sin_hour', 'cos_weekday', 'sin_weekday',
    'is_holiday',
]
time_series_cols = [
    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp',
    'cos_hour', 'sin_hour', 'cos_weekday', 'sin_weekday',
    'is_holiday', 'target',
]
target_cols = ['target']
to_inverse_cols = ['mean_to_inverse', 'std_to_inverse']
input_cols = list(set(
    building_num_cols + building_info_cols + target_time_info_cols +
    time_series_cols + target_cols + to_inverse_cols
))

CONFIGS['building_num_cols'] = building_num_cols
CONFIGS['building_info_cols'] = building_info_cols
CONFIGS['target_time_info_cols'] = target_time_info_cols
CONFIGS['time_series_cols'] = time_series_cols
CONFIGS['target_cols'] = target_cols
CONFIGS['to_inverse_cols'] = to_inverse_cols
CONFIGS['input_cols'] = input_cols
```

사용할 column들을 구분한다.  
지난번 multi-input에서 사용했던 `time_series_cols`와 `target_time_info_cols`에 더불어 건물번호와 새로만든 `building_info_cols`를 함께 넣어준다.  
(`to_inverse_cols`는 scaling된 target을 되돌리는데 사용할 예정이며 뒤에서 추가로 설명)

#### Crop

```python
def crop(data, crop_type, CONFIGS):
    building_length = tf.shape(data)[0]
    if crop_type == 'one_row':
        h = CONFIGS['target_length']//2
        croped = data[CONFIGS['window_size']+h:building_length-CONFIGS['target_length']+1+h]
    elif crop_type == 'time_series_input':
        croped = data[:-CONFIGS['target_length']]
    elif crop_type == 'target':
        croped = data[CONFIGS['window_size']:]
    return croped
```

각 column들은 사용을 어떻게 할지에 따라 사용하는 기간이 다르다.  
- `target_time_info_cols`나 `building_info_cols`같은 경우 시계열형태로 sequence가 들어가지 않고 하나의 row형태로 들어갈 것이기 때문에 `target_length = 3`의 중간데이터만을 활용한다. 따라서 앞에서 `window_size`만큼, 뒤에서 `target_length`만큼 잘라내고 가운데 부분만을 사용한다.
- `time_series_cols`는 sequence의 마지막 target은 input으로 들어가지 않기 때문에 해당 부분은 잘라낸다.
- `target`으로 사용하는 경우 첫부분의 `window_size`만큼은 input으로 들어가지 않기 때문에 잘라낸다.

#### Make Window

```python
def mk_window(data, size, shift):
    ds = Dataset.from_tensor_slices(data)
    ds = ds.window(
        size=size, shift=shift, drop_remainder=True
    ).flat_map(lambda x: x).batch(size)
    return ds
```

`time_series_cols`와 `target_cols`는 window 형태로 만드는 과정이 필요하다.  
자세한 내용은 [tf.data.Dataset을 이용한 시계열 데이터 구성](https://fidabspd.github.io/2022/01/10/time_series_2.html)참고

#### Make tf.data.Dataset

```python
def mk_time_series(data, building_length, crop_type, CONFIGS, dtype=None):
    if not dtype:
        dtype = CONFIGS['dtype']

    ds = Dataset.from_tensor_slices(data).batch(building_length)
    if crop_type == 'one_row':
        ds = ds.map(lambda x: crop(x, crop_type, CONFIGS))
        ds = ds.flat_map(lambda x: Dataset.from_tensor_slices(x))
    elif crop_type == 'time_series_input':
        ds = ds.map(lambda x: crop(x, crop_type, CONFIGS))
        ds = ds.flat_map(
            lambda x: mk_window(x, CONFIGS['window_size'], CONFIGS['shift']))
    elif crop_type == 'target':
        ds = ds.map(lambda x: crop(x, crop_type, CONFIGS))
        ds = ds.flat_map(
            lambda x: mk_window(x, CONFIGS['target_length'], CONFIGS['shift']))
    ds.map(lambda x: tf.cast(x, dtype))

    return ds


def mk_dataset(data, CONFIGS, shuffle=False):

    data = data[CONFIGS['input_cols']]
    building_length = data.shape[0]//60

    building_num = data[CONFIGS['building_num_cols']]
    building_info = data[CONFIGS['building_info_cols']]
    target_time_info = data[CONFIGS['target_time_info_cols']]
    time_series = data[CONFIGS['time_series_cols']]
    to_inverse = data[CONFIGS['to_inverse_cols']]
    target = data[CONFIGS['target_cols']]

    building_num_ds = mk_time_series(building_num, building_length, 'one_row', CONFIGS, tf.int16)
    building_info_ds = mk_time_series(building_info, building_length, 'one_row', CONFIGS)
    target_time_info_ds = mk_time_series(target_time_info, building_length, 'one_row', CONFIGS)
    time_series_ds = mk_time_series(time_series, building_length, 'time_series_input', CONFIGS)
    target_ds = mk_time_series(target, building_length, 'target', CONFIGS)
    to_inverse_ds = mk_time_series(to_inverse, building_length, 'one_row', CONFIGS)
    
    # zip
    ds = Dataset.zip((
        (
            building_num_ds,
            building_info_ds,
            target_time_info_ds,
            time_series_ds,
            to_inverse_ds
        ),
        target_ds
    ))
    if shuffle:
        ds = ds.shuffle(512)
    ds = ds.batch(CONFIGS['batch_size']).cache().prefetch(2)
    
    return ds
```

우선 data를 어떻게 활용할지 여부에 따라 column indexing을 진행하여 다른 DataFrame으로 만든다.  
그리고 나서 multi-task learning dataset구성의 키포인트. 각 건물의 sequence길이만큼을 batch로 잘라준다.  
예시를 보면 다음과 같다.

```python
building_length = new_data[CONFIGS['input_cols']].query('num == 0').shape[0]

ds = Dataset.from_tensor_slices(new_data[CONFIGS['input_cols']]).batch(building_length)
iter(ds).next()
```

```
<tf.Tensor: shape=(2040, 22), dtype=float64, numpy=
array([[ 1.        ,  0.        , -0.78192177, ...,  0.        ,
        -0.54498631,  3.4315275 ],
       [ 0.96592583,  0.        , -0.78192177, ...,  0.        ,
        -0.54498631,  3.4315275 ],
       [ 0.8660254 ,  0.        , -0.78192177, ...,  0.        ,
        -0.54498631,  3.4315275 ],
       ...,
       [ 0.70710678,  0.        , -0.78192177, ...,  0.        ,
        -0.54498631,  3.4315275 ],
       [ 0.8660254 ,  0.        , -0.78192177, ...,  0.        ,
        -0.54498631,  3.4315275 ],
       [ 0.96592583,  0.        , -0.78192177, ...,  0.        ,
        -0.54498631,  3.4315275 ]])>
```

`ds`는 위와 같이 길이 2040인 데이터 60개로 잘리게 된다.  
이제부터는 각 건물 60개에 대해 [Multi-input 시계열 모델](https://fidabspd.github.io/2022/01/18/time_series_3.html)에서 진행했던 것과 같이 데이터를 구성하면 된다. (`map`을 이용한다.)

- 앞서 만든 `crop`을 이용하여 사용하는 데이터만 남기고 자른다.
- 하나의 row형태로 활용하는 data는 `flat_map`과 `Dataset.from_tensor_slices`를 이용하여 각각의 건물들을 또다시 데이터 셋으로 만들고 flatten 해준다.
- window형태로 활용하는 data는 `flat_map`과 앞서 만든 `mk_window`를 이용하여 각각의 건물들에 대해 window를 만들고 flatten 해준다.

여기까지가 각각의 건물들에 따로 적용해주는 부분이다.  
이제부터는 모델의 input으로 들어갈 수 있도록 `zip`으로 묶고, `shuffle`, `batch`등을 해주는 작업이 필요하다.

- `.map(lambda x: tf.cast(x, dtype))`: 데이터 타입을 바꿔준다. (꼭 필요한 과정은 아님. 사이즈가 작은 dtype 이용시 효율적인 메모리 사용과 속도향상을 기대할 수 있음.)
- `zip`을 이용하여 데이터를 묶는다.
- 필요하다면 `shuffle`을 이용한다.
- `batch`, `cache`, `prefetch`를 적용한다. (하나 건물의 sequence가 현재 약 1500이 넘기 때문에 `shuffle`의 `buffer_size`를 이보다는 늘려주는 것이 좀 더 잘 shuffle하는 것이지만 일단은 그냥 진행.)

#### Train, Valid, Test Split & Apply mk_dataset

```python
str_to_dt = lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H')
hour_to_td = lambda x: datetime.timedelta(hours=x)

train = new_data.loc[
    new_data['date_time'] < \
        str_to_dt(CONFIGS['valid_start_date_time']),
    :
]
valid = new_data.loc[
    (new_data['date_time'] >= \
        str_to_dt(CONFIGS['valid_start_date_time'])-hour_to_td(CONFIGS['window_size']))&\
    (new_data['date_time'] < \
         str_to_dt(CONFIGS['test_start_date_time'])),
    :
]
test = new_data.loc[
    new_data['date_time'] >= \
        str_to_dt(CONFIGS['test_start_date_time'])-hour_to_td(CONFIGS['window_size']),
    :
]

train_ds = mk_dataset(train, CONFIGS, shuffle=True)
valid_ds = mk_dataset(valid, CONFIGS)
test_ds = mk_dataset(test, CONFIGS)
```

미리 정해놓은 train, valid, test split 기준에 따라 `new_data`를 분리한다.  

`mk_dataset` 적용 결과는 다음과 같다.

```python
print(train_ds)
```

```
<PrefetchDataset shapes: (((None, 1), (None, 7), (None, 11), (None, None, 12), (None, 2)), (None, None, 1)), types: ((tf.int16, tf.float32, tf.float32, tf.float32, tf.float32), tf.float32)>
```

## 분량 조절 실패로 Modeling 파트는 2부에서 계속..

## 목차

1. [시계열 데이터의 기본적인 특징과 간단한 모델](https://fidabspd.github.io/2022/01/04/time_series_1.html)
1. [tf.data.Dataset을 이용한 시계열 데이터 구성](https://fidabspd.github.io/2022/01/10/time_series_2.html)
1. [Multi-Input 시계열 모델](https://fidabspd.github.io/2022/01/18/time_series_3.html)
1. [**Multi-Task Learning 시계열 모델 (1)**](https://fidabspd.github.io/2022/01/28/time_series_4-1.html)
1. [Multi-Task Learning 시계열 모델 (2)](https://fidabspd.github.io/2022/01/30/time_series_4-2.html)
1. [시계열 target의 결측](https://fidabspd.github.io/2022/02/10/time_series_5.html)
1. [Recursive Input Prediction](https://fidabspd.github.io/2022/02/17/time_series_6.html)

## 원본 코드 ➞ [<span style="color:#AC1538">CODE (GitHub)</span>](https://github.com/fidabspd/time_series/tree/master/codes/4_multi_task_learning.ipynb)
