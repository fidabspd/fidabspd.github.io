---
layout: post
title: Time Series - Multi-Task Learning 시계열 모델 (2)
tags: [Time-Series, Tensorflow, Keras]
excerpt_separator: <!--more-->
---

[Multi-Task Learning 시계열 모델 1부](https://fidabspd.github.io/2022/01/28/time_series_4-1.html)에 이은 2부.  
Loss와 Metric, Layer를 customize하는 방법과, Multi-Task Learning의 Modeling에 대해 다뤄보자.
<!--more-->

## 목차

1. 시계열 데이터의 기본적인 특징과 간단한 모델
1. tf.data.Dataset을 이용한 시계열 데이터 구성
1. Multi-Input 시계열 모델
1. Multi-Task Learning 시계열 모델 (1)
1. **Multi-Task Learning 시계열 모델 (2)**
1. 시계열 target의 결측
1. 이전의 예측값을 다음의 input으로 recursive하게 이용

## 원본 코드 ➞ [<span style="color:#AC1538">CODE (GitHub)</span>](https://github.com/fidabspd/time_series/tree/master/codes/4_multi_task_learning.ipynb)

***

## 시작하기 전에

- **본 포스팅 시리즈의 목적은 시계열 예측의 정확성을 높이기 위함이 아니다.**
- **단지 시계열 데이터를 다루고 예측함에 있어 통상적으로 부딪히는 여러 문제들과 그를 어떻게 해결하면 좋을 지 가이드를 제시하는 것에 목적이 있다.**
- **데이터는 [DACON 전력사용량 예측 AI 경진대회](https://dacon.io/competitions/official/235736/overview/description)의 데이터를 사용한다.** 감사합니다 DACON!
    - 60개의 건물의 전력 수요량을 미리 예측하는 것이 목적이며, 본 대회는 그를 위한 데이터를 다음과 같이 제공한다.
    - 데이터 column 구성
        - `num`: 건물 번호
        - `date_time`: 데이터가 측정된 시간 (1시간 단위)
        - `target`: 전력 사용량 (예측해야하는 값)
        - `temp`: 기온(°C)
        - `wind`: 풍속(m/s)
        - `humid`: 습도(%)
        - `rain`: 강수량(mm)
        - `sun`: 일조(hr)
        - `non_elec_eq`: 건물별 비전기 냉방 설비 운영 여부 (1: 보유, 0: 미보유)
        - `sunlight_eq`: 건물별 태양광 설비 보유 여부 (1: 보유, 0: 미보유)
    - **이번 포스팅에서는 모든 데이터를 사용**

***

## CODE

[Multi-Task Learning 시계열 모델 (1)](https://fidabspd.github.io/2022/01/28/time_series_4-1.html)에서 train, valid, test Dataset을 만드는 것까지 마쳤으며 그에 바로 이어지는 내용이다.

### Modeling

#### Customize Loss

현재 `target`은 각 건물의 데이터들을 따로 기준삼아 standard scaling되어있다. 이를 그대로 두고 loss를 계산하고 backpropagation을 진행할 경우 `target`평균 값이 10000인 건물의 loss 1000과 평균값이 1000인 건물의 loss 100은 비슷한 영향을 미치게 된다. 하지만 본 모델의 목적은 전체 loss를 잘 맞추는 것이기 때문에 이에 대한 보정으로 각 건물들에서 발생하는 loss에 대한 일종의 가중치 연산 작업이 필요하다.

각 건물들에서 발생하는 loss에 가중치를 주는 방법은 정말 다양하다. loss들에 각 건물 `target`의 평균값을 곱해주는 것으로도 간단하게 해결 가능하다. 하지만 지금은 최대한 직관적으로 계산하기 위해 scaling된 `target`을 원래대로 되돌려 `inversed_target`을 만들어주고 이를 이용해 loss를 계산하고 모든 건물의 `target`중 최대값으로 나누어주도록 하겠다.

이쯤에서 드는 의문점이 있다. *'scaling한거 되돌려서 다시 그냥 loss 계산할거였으면 애당초 scaling 안했으면 이런 번거로운거 할필요 없던거 아닌가?'*  
사실이다. 특별히 standard scaling을 하지 않고 loss overflow를 막아주기 위해 모든 건물의 `target`중 최대값으로 나눈 값을 이용해 loss를 계산해도 충분하긴 하다. 그래도 모델은 잘 작동할 것이다.  
하지만 시계열 예측인 만큼 `target`으로 사용되고 있는 `전력 사용량`은 input으로도 사용된다. 이 input되는 값들의 scale이 task(건물)마다 다르다면 multi-task learning의 효과가 떨어질 수 있다.

그럼 또하나의 의문점. *'그럼 input으로 들어가는 전력 사용량만 scaling해주고 target으로 사용되는건 그냥 두면 되는거 아닌가?'*  
이 또한 사실이다. 하지만 이렇게하면 각 건물의 scaling 정보(standard scaling에서는 mean, std) 또한 모델이 직접 학습해야한다. 모델의 부담이 늘어난다고 봐도 좋다. 정답이 정해져있는건 알려주는 것이 좋다.

참고로 이렇게 loss를 custom해주는 것이나 model layer의 마지막에서 inverse를 해주나 모델에 scaling 정보를 알려주게되는 효과는 같을 것이다.  
하지만 우선은 loss를 custom하기로 결심했기 때문에 이렇게 진행한다.

```python
class CustomMSE(Loss):
    
    def __init__(self, target_max, name="custom_mse"):
        super(CustomMSE, self).__init__(name=name)
        self.target_max = target_max

    def call(self, y_true, y_pred):
        y_true = tf.squeeze(y_true)
        mean = tf.reshape(y_pred[:, -2], (-1, 1))
        std = tf.reshape(y_pred[:, -1], (-1, 1))
        y_pred = y_pred[:, :-2]

        y_true_inversed = y_true*std+mean
        y_pred_inversed = y_pred*std+mean
        
        y_true_inversed_scaled = y_true_inversed/self.target_max
        y_pred_inversed_scaled = y_pred_inversed/self.target_max

        mse = tf.reduce_mean((y_true_inversed_scaled-y_pred_inversed_scaled)**2)
        return mse
```

`tensorflow.keras.losses.Loss`를 상속받아 loss를 cutomize하는 방법을 모른다면 생뚱맞게 느껴질텐데 현재는 간단하게만 설명하도록 하겠다.  
`tensorflow.keras.losses.Loss`를 상속받은 class에 `call` method를 선언하면 이를 이용해 loss를 계산하게 된다. `call`은 기본적인 `__call__`과 같은 기능을 한다. 
예컨데 위의 코드는 다음과 같이 사용이 가능하다.

```python
custom_mse = CustomMSE(target_max)
custom_mse(y_true, y_pred)
```

tensorflow 및 keras에서 Loss를 customize하는 방법은 언젠가 다른 포스팅에서 좀 더 다뤄보도록 하겠다.

이제 loss 식을 살펴보겠다.  
모델 구성을 보지 않고 당장 코드만 보면 '이게 뭐지' 싶을것이다.
Dataset 구성에서 뒷부분에 설명한다고 했던 `mean_to_inverse`와 `std_to_inverse`가 여기에 쓰인다.  
모델 구성 부분에서 `mean_to_inverse`와 `std_to_inverse` 두개의 column을 예측값에 `Concatenate`해주어 output에 포함시킬 것이다. 그렇게 `y_pred`의 마지막 column두개는 각각 `mean_to_inverse`와 `std_to_inverse`가 된다. 두개를 이용하여 standard scaling 된 값을 inverse해준다.  
그렇게 inverse된 값들을 loss 계산 과정에서 overflow가 생기지 않도록 모든 건물의 `target`값의 max로 나눠주고 나서 mse를 계산한다.  
이렇게 하면 각 건물의 `mean`과 `std`가 loss의 계산과 각 weight들을 이용한 loss의 미분과정에 사용된다.(좀 더 정확히 말하자면 미분에는 `std`만 사용)  

위와 같이 loss를 customize함으로서 각 건물 `target`의 scale 차이를 반영할 수 있다.

#### Customize Metric

`inversed_rmse`를 계산하기위해 metric을 customize해준다.

앞선 시계열 시리즈 포스팅들에서는 custom metric을 아래와 같이 정말 간단하게 만들었다.

```python
def inversed_rmse(y_true, y_pred, mean, std):
    y_true = y_true*std+mean
    y_pred = y_pred*std+mean
    mse = tf.reduce_mean((y_true-y_pred)**2)
    return tf.sqrt(mse)

inversed_rmse_metric = lambda y_true, y_pred: inversed_rmse(y_true, y_pred, **CONFIGS['mean_std_dict']['target'])
```

하지만 사실 이는 잘못 만들어진 metric이다.  
결론부터 말하자면 valid와 test에는 잘 작동하지만 훈련과정중 train에는 제대로 작동하지 않는다.  
이유는 다음과 같다.

원래 RootMeanSquaredError는 모든 loss들을 전부 평균낸 뒤에 Root를 취해준다. 하지만 위의 metric은 batch마다 평균을 내고 Root를 취하고 다시 그 값을 평균내게 되어있다.  
따라서 훈련중 계산되는 train metric은 잘못된 값이다. 데이터셋 전체를 한번에 넣어줘야 제대로 작동한다. (이 이유로 valid와 test에서는 제대로 작동한다.)

Loss를 customize하는김에 Metric까지 제대로 customize해보자.

```python
class InversedRMSE(Metric):
    
    def __init__(self, CONFIGS, name="inversed_rmse", **kwargs):
        super(InversedRMSE, self).__init__(name=name, **kwargs)
        self.inversed_mse = self.add_weight(name='inversed_mse', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
        self.CONFIGS = CONFIGS

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.reshape(y_true, (-1, CONFIGS['target_length']))
        mean = tf.reshape(y_pred[:, -2], (-1, 1))
        std = tf.reshape(y_pred[:, -1], (-1, 1))
        y_pred = y_pred[:, :-2]

        y_true_inversed = y_true*std+mean
        y_pred_inversed = y_pred*std+mean

        error = tf.reduce_sum(tf.math.squared_difference(y_true_inversed, y_pred_inversed))
        
        self.inversed_mse.assign_add(error)
        self.count.assign_add(tf.cast(tf.size(y_true), CONFIGS['dtype']))

    def result(self):
        return tf.sqrt(tf.math.divide_no_nan(self.inversed_mse, self.count))
```

custom loss때와 마찬가지로 `tensorflow.keras.metrics.Metric`을 상속받아 metric을 customize해본 적이 없다면 생뚱맞게 느껴질 것이다. 간단하게만 설명해보겠다.  
`update_state`와 `result`는 무조건 만들어줘야하는 method이다. (안만들면 에러난다. 직접 코드를 보면 부모 클래스인 `Metric`에 추상클래스로 `NotImplementedError`가 있다.)  

- `__init__`에서 metric 계산에 필요한 값들을 선언한다.
- `update_state`에서 값들을 계산한다.
- `result`는 최종적으로 출력되는 metric 값이다.

구글링 해봐도 `tensorflow.keras.metrics.Metric`을 상속받아 metric을 customize하는 방법에 대한 글은 잘 나와있지 않다.  
이도 Loss customize에 대한 포스팅을 할때 자세한 내용을 적어보겠다.

#### Customize Layer

솔직히 굳이 안해도 된다. 앞선 시계열 시리즈 포스팅들에서는 안해주고도 코드 잘 작동했다.  
다만 해주면 코드가 좀 더 깔끔하고 TensorBoard의 Graphs도 더 정돈된 모양으로 볼 수 있다.  
(이 역시도 `Loss`, `Metric`과 마찬가지로 다른 포스팅에서 좀 더 자세히 설명해보겠다.)

Layer를 Customize해주는 방법은 우선 `tensorflow.keras.layers.Layer`를 상속받은 클래스를 만들고 그 안에 `__init__`, `get_config`, `call` method를 만든다. (`get_config`는 꼭 필수는 아니다. 이 또한 Layer custom에 대해 자세히 다루게 되는 포스팅에서 설명 예정.)  
간단한 Dense block 레이어를 정의한 코드를 살펴보자.  

```python
class DenseBlock(Layer):
    def __init__(self):
        super(DenseBlock, self).__init__()
        self.dense_0 = Dense(32)
        self.dense_1 = Dense(16)
        
    def get_config(self):
        config = super(DenseBlock, self).get_config().copy()
        config.update({
            'dense_0': self.dense_0,
            'dense_1': self.dense_1,
        })
        return config
        
    def call(self, inputs):
        x = self.dense_0(inputs)
        return self.dense_1(x)

dense_block = DenseBlock()

result = dense_block(np.arange(10).reshape((2, 5)))
print(result)
```

```
tf.Tensor(
[[-0.4134485   3.5550003   0.36978394 -1.269461    4.3724923  -0.13032854
   2.6228726   3.6299894  -3.3431456   1.7908558   0.7053635  -1.4296708
  -1.9010522  -3.4853628   1.691452    1.1247232 ]
 [-1.052686   10.318264    4.193324   -2.6979067  12.4428215  -2.943272
   6.207938   11.865471   -9.990765    8.823524    1.8229105  -2.3006048
  -6.597555   -7.8022175   4.197058    0.9843228 ]], shape=(2, 16), dtype=float32)
```

아주 직관적으로 이해 가능하다.  
그냥 모델 set하는 과정에서 줄줄이 만들었던걸 덩어리로 묶어주는 정도로 생각하면 편하다.

#### Building Number

각 건물번호를 처리하는 Layer를 만든다.

각 건물번호 그 자체로 input을 넣어준다면 당연히 안될것이 뻔하다.  
이를 input으로 활용하는 방법은 다양하겠지만 우선 두가지정도만 이야기 해보자면

- 건물 번호를 각각 One-Hot Encoding 해준다. (60개의 column이 추가된다.)
- Embedding을 활용하여 새로운 가중치로 학습한다. (embedding output dimension 만큼의 column이 추가된다.)

첫번째 방법도 현재 케이스에서는 나쁘지 않다고 생각한다. 60개 건물정도야 One-Hot Encoding 해줘도 괜찮을 것이다.  
하지만 task(현재는 건물)이 많아진다면? 2000개가 되면?  
One-Hot Encoding으로 인해 늘어나는 column이 감당이 안될 것이다.  

Embedding을 진행해보자.  
(Embedding의 개념 자체에 대해 다루진 않는다.)

```python
class BuildingNum(Layer):

    def __init__(self, CONFIGS, name='building_num_layer', **kwargs):
        super(BuildingNum, self).__init__(name=name, **kwargs)
        self.building_num_emb = Embedding(
            input_dim=CONFIGS['n_buildings'],
            output_dim=CONFIGS['embedding_dim']
        )
        self.bn_outputs = Reshape(target_shape=(CONFIGS['embedding_dim'],))
        
    def get_config(self):
        config = super(BuildingNum, self).get_config().copy()
        config.update({
            'building_num_emb': self.building_num_emb,
            'bn_outputs': self.bn_outputs,
        })
        return config
        
    def call(self, inputs):
        x = self.building_num_emb(inputs)
        outputs = self.bn_outputs(x)
        return outputs
```

- `Embedding`레이어를 활용
- `input_dim`은 당연히 건물의 개수
- `output_dim`은 적당히 선택
- `Embedding`레이어를 거치면 2차원 형태를 반환하기 때문에 1차원 형태로 변형 (숫자가 한개씩밖에 들어가지 않기때문에 괜찮다.)

#### Building Information

각 건물들이 고유하게 가지는 정보들을 처리한다.  
하나의 데이터마다 하나의 row를 가지는 단순한 형태이기 때문에 `Dense`를 이용하여 만든다.  
앞선 시계열 시리즈들과 다르게 모델이 많이 복잡해졌기 때문에 간단하게 `Dropout`도 활용해보았다.

```python
class BuildingInfo(Layer):
    
    def __init__(self, CONFIGS, name='building_info_layer', **kwargs):
        super(BuildingInfo, self).__init__(name=name, **kwargs)
        self.bi_dense_0 = Dense(16, activation='relu')
        self.dropout_0 = Dropout(0.3)
        self.bi_outputs = Dense(32, activation='relu')
        
    def get_config(self):
        config = super(BuildingInfo, self).get_config().copy()
        config.update({
            'bi_dense_0': self.bi_dense_0,
            'dropout_0': self.dropout_0,
            'bi_outputs': self.bi_outputs,
        })
        return config
        
    def call(self, inputs):
        x = self.bi_dense_0(inputs)
        x = self.dropout_0(x)
        outputs = self.bi_outputs(x)
        return outputs
```

#### Target Time Information

마찬가지로 `Dense`활용.

```python
class TargetTimeInfo(Layer):
    
    def __init__(self, CONFIGS, name='target_time_info_layer', **kwargs):
        super(TargetTimeInfo, self).__init__(name=name, **kwargs)
        self.tti_dense_0 = Dense(16, activation='relu')
        self.dropout_0 = Dropout(0.3)
        self.tti_outputs = Dense(32, activation='relu')
        
    def get_config(self):
        config = super(TargetTimeInfo, self).get_config().copy()
        config.update({
            'tti_dense_0': self.tti_dense_0,
            'dropout_0': self.dropout_0,
            'tti_outputs': self.tti_outputs,
        })
        return config
        
    def call(self, inputs):
        x = self.tti_dense_0(inputs)
        x = self.dropout_0(x)
        outputs = self.tti_outputs(x)
        return outputs
```

#### Time Series

앞선 시계열 시리즈 포스팅에서 시계열을 처리할 때와 달라진 점은 없다.  
옵션에 따라 `flatten`, `cnn1d`, `cnn2d`, `lstm`, `bilstm` 중에 선택 가능하다.

```python
class TimeSeries(Layer):
    
    def __init__(self, CONFIGS, name='time_series_layer', **kwargs):
        super(TimeSeries, self).__init__(name=name, **kwargs)
        
        if CONFIGS['model_type'] == 'flatten':
            pass
        elif CONFIGS['model_type'] == 'cnn1d':
            self.conv1d_0 = Conv1D(16, 3, 2, activation='relu')
            self.pool1d_0 = MaxPool1D(2)
            self.conv1d_1 = Conv1D(32, 3, 2, activation='relu')
            self.pool1d_1 = MaxPool1D(2)
        elif CONFIGS['model_type'] == 'cnn2d':
            self.conv2d_reshape = Reshape(target_shape=(
                CONFIGS['window_size'], len(CONFIGS['time_series_cols']), 1
            ))
            self.conv2d_0 = Conv2D(8, (3, 1), strides=(2, 1), activation='relu')
            self.pool2d_0 = MaxPool2D((2, 1))
            self.conv2d_1 = Conv2D(16, (3, 1), strides=(2, 1), activation='relu')
            self.pool2d_1 = MaxPool2D((2, 1))
        elif CONFIGS['model_type'] == 'lstm':
            self.lstm_0 = LSTM(16, return_sequences=True, activation='relu')
            self.lstm_1 = LSTM(32, activation='relu')
        elif CONFIGS['model_type'] == 'bilstm':
            self.bilstm_0 = Bidirectional(LSTM(16, return_sequences=True, activation='relu'))
            self.bilstm_1 = Bidirectional(LSTM(32, activation='relu'))
        self.time_series_outputs = Flatten()
        
    def get_config(self):
        config = super(TimeSeries, self).get_config().copy()
        if CONFIGS['model_type'] == 'flatten':
            pass
        elif CONFIGS['model_type'] == 'cnn1d':
            config.update({
                'conv1d_0': self.conv1d_0,
                'pool1d_0': self.pool1d_0,
                'conv1d_1': self.conv1d_1,
                'pool1d_1': self.pool1d_1,
            })
        elif CONFIGS['model_type'] == 'cnn2d':
            config.update({
                'conv2d_reshape': self.conv2d_reshape,
                'conv2d_0': self.conv2d_0,
                'pool2d_0': self.pool2d_0,
                'conv2d_1': self.conv2d_1,
                'pool2d_1': self.pool2d_1,
            })
        elif CONFIGS['model_type'] == 'lstm':
            config.update({
                'lstm_0': self.lstm_0,
                'lstm_1': self.lstm_1,
            })
        elif CONFIGS['model_type'] == 'bilstm':
            config.update({
                'bilstm_0': self.bilstm_0,
                'bilstm_1': self.bilstm_1,
            })
        config.update({
            'time_series_outputs': self.time_series_outputs,
        })
        return config
        
    def call(self, inputs):
        if CONFIGS['model_type'] == 'flatten':
            x = inputs
        elif CONFIGS['model_type'] == 'cnn1d':
            x = self.conv1d_0(inputs)
            x = self.pool1d_0(x)
            x = self.conv1d_1(x)
            x = self.pool1d_1(x)
        elif CONFIGS['model_type'] == 'cnn2d':
            x = self.conv2d_reshape(x)
            x = self.conv2d_0(x)
            x = self.pool2d_0(x)
            x = self.conv2d_1(x)
            x = self.pool2d_1(x)
        elif CONFIGS['model_type'] == 'lstm':
            x = self.lstm_0(x)
            x = self.lstm_1(x)
        elif CONFIGS['model_type'] == 'bilstm':
            x = self.bilstm_0(x)
            x = self.bilstm_1(x)
        outputs = self.time_series_outputs(x)
        return outputs
```

#### Set Model

```python
def set_model(CONFIGS, model_name=None, print_summary=False):
    
    # building_num
    building_num_inputs = Input(batch_shape=(None, 1), name='building_num_inputs')
    bn_outputs = BuildingNum(CONFIGS)(building_num_inputs)
    
    # building_info
    building_info_inputs = Input(
        batch_shape=(None, len(CONFIGS['building_info_cols'])),
        name='building_info_inputs'
    )
    bi_outputs = BuildingInfo(CONFIGS)(building_info_inputs)
    
    # target_time_info
    target_time_info_inputs = Input(
        batch_shape=(None, len(CONFIGS['target_time_info_cols'])),
        name='target_time_info_inputs'
    )
    tti_outputs = TargetTimeInfo(CONFIGS)(target_time_info_inputs)
    
    # time_series
    time_series_inputs = Input(batch_shape=(
        None, CONFIGS['window_size'], len(CONFIGS['time_series_cols'])
    ), name='time_series_inputs')
    time_series_outputs = TimeSeries(CONFIGS)(time_series_inputs)
    
    concat = Concatenate(name='concat')([bn_outputs, bi_outputs, tti_outputs, time_series_outputs])
        
    dense_0 = Dense(64, activation='relu', name='dense_0')(concat)
    dropout_0 = Dropout(0.3, name='dropout_0')(dense_0)
    dense_1 = Dense(32, activation='relu', name='dense_1')(dropout_0)
    dropout_1 = Dropout(0.3, name='dropout_1')(dense_1)
    outputs = Dense(CONFIGS['target_length'], name='outputs')(dropout_1)
    
    # to_inverse
    to_inverse_inputs = Input(batch_shape=(None, len(CONFIGS['to_inverse_cols'])), name='to_inverse_inputs')
    concat_to_inverse = Concatenate(name='concat_to_inverse')([outputs, to_inverse_inputs])
    
    if not model_name:
        model_name = CONFIGS['model_name']
    
    model = Model(
        inputs = [
            building_num_inputs,
            building_info_inputs,
            target_time_info_inputs,
            time_series_inputs,
            to_inverse_inputs
        ],
        outputs = concat_to_inverse,
        name = model_name
    )
    
    custom_mse = CustomMSE(CONFIGS['target_max'])
    inversed_rmse = InversedRMSE(CONFIGS)
    optimizer = Adam(learning_rate=CONFIGS['learning_rate'])
    model.compile(
        loss = custom_mse,
        optimizer = optimizer,
        metrics = [inversed_rmse],
    )
    
    if print_summary:
        model.summary()
    
    return model


CONFIGS['target_max'] = \
    data[data['date_time']<CONFIGS['valid_start_date_time']]['target'].max()
CONFIGS['embedding_dim'] = 10

model = set_model(CONFIGS, print_summary=True)
```

```
Model: "multi_task_learning"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 building_num_inputs (InputLaye  [(None, 1)]         0           []                               
 r)                                                                                               
                                                                                                  
 building_info_inputs (InputLay  [(None, 7)]         0           []                               
 er)                                                                                              
                                                                                                  
 target_time_info_inputs (Input  [(None, 11)]        0           []                               
 Layer)                                                                                           
                                                                                                  
 time_series_inputs (InputLayer  [(None, 168, 12)]   0           []                               
 )                                                                                                
                                                                                                  
 building_num_layer (BuildingNu  (None, 10)          600         ['building_num_inputs[0][0]']    
 m)                                                                                               
                                                                                                  
 building_info_layer (BuildingI  (None, 32)          672         ['building_info_inputs[0][0]']   
 nfo)                                                                                             
                                                                                                  
 target_time_info_layer (Target  (None, 32)          736         ['target_time_info_inputs[0][0]']
 TimeInfo)                                                                                        
                                                                                                  
 time_series_layer (TimeSeries)  (None, 320)         2160        ['time_series_inputs[0][0]']     
                                                                                                  
 concat (Concatenate)           (None, 394)          0           ['building_num_layer[0][0]',     
                                                                  'building_info_layer[0][0]',    
                                                                  'target_time_info_layer[0][0]', 
                                                                  'time_series_layer[0][0]']      
                                                                                                  
 dense_0 (Dense)                (None, 64)           25280       ['concat[0][0]']                 
                                                                                                  
 dropout_0 (Dropout)            (None, 64)           0           ['dense_0[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 32)           2080        ['dropout_0[0][0]']              
                                                                                                  
 dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                
                                                                                                  
 outputs (Dense)                (None, 3)            99          ['dropout_1[0][0]']              
                                                                                                  
 to_inverse_inputs (InputLayer)  [(None, 2)]         0           []                               
                                                                                                  
 concat_to_inverse (Concatenate  (None, 5)           0           ['outputs[0][0]',                
 )                                                                'to_inverse_inputs[0][0]']      
                                                                                                  
==================================================================================================
Total params: 31,627
Trainable params: 31,627
Non-trainable params: 0
__________________________________________________________________________________________________
```

**TensorBoard GRAPHS**

TensorBoard의 GRAPHS에서 모델구조만을 캡쳐한 사진이다.

![model_structure](/assets/img/posts/time_series_4-2/model_structure.png)

Customize 해준 각 레이어들을 활용하여 나온 output들을 `Concatenate`하여 `Dense`를 통해 `target_length`만큼의 output을 뽑아내는 것에는 크게 다름이 없다.

그런데 한가지 다른점이 있다.  
바로 이부분

```python
    # to_inverse
    to_inverse_inputs = Input(batch_shape=(None, len(CONFIGS['to_inverse_cols'])), name='to_inverse_inputs')
    concat_to_inverse = Concatenate(name='concat_to_inverse')([outputs, to_inverse_inputs])
```

[Customize Loss](#customize-loss)에서 **모델 구성 부분에서 `mean_to_inverse`와 `std_to_inverse` 두개의 column을 예측값에 `Concatenate`해주어 output에 포함시킬 것이다.** 했던 부분이 바로 이것이다.  
이렇게 최종 output에 `mean_to_inverse`와 `std_to_inverse` 두개의 column을 `Concatenate`해주었기 때문에 최종 output의 column개수는 `target_length + 2`가 된다.

Customize 해준 `Loss`와 `Metric`도 `model.compile`에 사용해준다.

#### Train

```python
def train_model(model, train_ds, valid_ds, CONFIGS):
    
    early_stop = EarlyStopping(
        patience=CONFIGS['es_patience']
    )
    save_best_only = ModelCheckpoint(
        filepath = f'{CONFIGS["model_path"]}{CONFIGS["model_name"]}.h5',
        monitor = 'val_loss',
        save_best_only = True,
        save_weights_only = True
    )
    tensorboard_callback = TensorBoard(
        log_dir = CONFIGS['tensorboard_log_path']
    )
    
    history = model.fit(
        train_ds,
        batch_size = CONFIGS['batch_size'],
        epochs = CONFIGS['epochs'],
        validation_data = valid_ds,
        callbacks = [
            early_stop,
            save_best_only,
            tensorboard_callback,
        ]
    )
    
    return history


history = train_model(model, train_ds, valid_ds, CONFIGS)
```

간단하게 `TensorBoard`도 활용해주었다.

#### Evaluate

```python
best_model = set_model(CONFIGS, model_name='best_'+CONFIGS['model_name'])
best_model.load_weights(f'{CONFIGS["model_path"]}{CONFIGS["model_name"]}.h5')

train_loss, train_rmse = best_model.evaluate(train_ds, verbose=0)
valid_loss, valid_rmse = best_model.evaluate(valid_ds, verbose=0)
test_loss, test_rmse = best_model.evaluate(test_ds, verbose=0)

print(f'train_loss: {train_loss:.6f}\ttrain_rmse: {train_rmse:.6f}')
print(f'valid_loss: {valid_loss:.6f}\tvalid_rmse: {valid_rmse:.6f}')
print(f'test_loss: {test_loss:.6f}\ttest_rmse: {test_rmse:.6f}')
```

```
train_loss: 0.000167	train_rmse: 218.776993
valid_loss: 0.000526	valid_rmse: 387.981140
test_loss: 0.000442	test_rmse: 355.469086
```

이렇게 60개 건물의 모든 데이터를 활용한 multi-task learning 모델을 완성하였다.

다음 포스팅에는 시계열 `target`값에 결측이 있을 때의 상황에 대해 다뤄보자.

## 목차

1. 시계열 데이터의 기본적인 특징과 간단한 모델
1. tf.data.Dataset을 이용한 시계열 데이터 구성
1. Multi-Input 시계열 모델
1. Multi-Task Learning 시계열 모델 (1)
1. **Multi-Task Learning 시계열 모델 (2)**
1. 시계열 target의 결측
1. 이전의 예측값을 다음의 input으로 recursive하게 이용

## 원본 코드 ➞ [<span style="color:#AC1538">CODE (GitHub)</span>](https://github.com/fidabspd/time_series/tree/master/codes/4_multi_task_learning.ipynb)
