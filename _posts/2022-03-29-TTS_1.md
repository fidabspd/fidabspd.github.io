---
layout: post
title: TTS - Text Preprocessing & Data Cleansing
tags: [TTS, Transformer, PyTorch]
excerpt_separator: <!--more-->
use_math: true
---

개인적인 사정이 여럿 겹쳐 진행이 많이 늦어졌다. 짬짬히라도 열심히 달려보자.

처음부터 다중화자 TTS를 향해 달려가면 조금 지칠 수 있으니 처음에는 Transformer를 이용하여 단일화자 TTS를 만드는 것을 1차 목표로 하고 진행한다.

Transformer로 챗봇 만들어봤으니 **이제 스크립트-음성 짝지어서 그대로 데이터만 바꾸면 TTS 만들 수 있겠다.** 라고 착각하기에는 몇가지 큰 걸림돌들이 있다.<!--more-->

## 의문점

간단하게 몇가지 의문점을 적어보면 다음과 같다.

- Text는 단어 임베딩을 하는 것이 맞는 것일까
- Text의 숫자, 기호, 영어 등은 어떻게 처리해야할까
- Speech의 Sequence 길이는 감당할 수 있는 길이일까
- Speech를 정수로 Tokenizing을 할 수 있을까

# Paper - Neural Speech Synthesis with Transformer Network

그리고 이를 위해 다음 논문을 참고한다.  

[**Neural Speech Synthesis with Transformer Network**](https://arxiv.org/abs/1809.08895)

## Architecture

기존의 기본적인 Transformer 아키텍쳐와 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895) 논문에서 제안하는 아키텍쳐를 비교하여 살펴보자.  
왼쪽이 [Attention Is All You Need](https://arxiv.org/abs/1706.03762)에서 제안한 기본적인 Transformer 아키텍쳐이고,  
오른쪽이 Transformer를 이용한 TTS 구현을 위한 논문 [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)에서 제안하는 아키텍쳐이다.

![architecture_comparison](/assets/img/posts/TTS_1/architecture_comparison.png)

기본적인 Transformer의 뼈대는 일치하지만 조금 다른 부분이 몇가지 보인다.  
나열해보자면 다음과 같다.

- Encoder와 Decoder의 Input 둘 모두 `Pre-net`이라는 레이어를 지난 뒤 사용된다.
- Decoder의 Output이 두갈래로 나뉘어 `Mel Linear`, `Stop Linear` 두개의 레이어의 Input으로 사용된다.
- `Mel Linear` 이후에 `Post-net`이 추가로 사용된다.
- Linear를 지난 결과가 `Softmax`에 들어가지 않는다.

[Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)에서 추가된 점을 염두에 두고 TTS에 대해 접근해보자.

# Data

우선은 Text-Speech 쌍지어진 데이터가 필요하다.  
어디에서 데이터를 구하면 좋을지, 어떤 데이터를 사용하면 될지 고민하다가 최종 목표인 다중화자 TTS에 굉장히 적합한 데이터를 찾았다.

[**AI Hub - 카이스트 오디오북 데이터셋**](https://aihub.or.kr/opendata/kaist-audiobook)  
좋은 데이터 공유해주신 카이스트. 정말 감사합니다.

AI Hub의 데이터를 우분투에 다운로드 받기 위해서는 별도의 프로그램 설치가 필요하다.  
[여기](https://aihub.or.kr/sites/default/files/FAQ%20%EC%8B%A0%EA%B7%9C%EB%93%B1%EB%A1%9D/INNORIX-EX-Ubuntu%EC%9A%A9_%EA%B0%84%EB%8B%A8%20%EC%82%AC%EC%9A%A9%20%EC%84%A4%EB%AA%85%EC%84%9C_%EC%9B%B9%EA%B3%B5%EA%B0%9C%EC%9A%A9.pdf)에 자세하게 설명 되어 있으니 참고하면 좋다.

위 데이터는 남자 7명, 여자 6명. 총 13명의 목소리로 이루어져있으며  
뉴스, 동화, 소설, 자기계발, 어학 카테고리에 대한 텍스트를 읽고 녹음한 데이터 셋이다.  
모든 화자가 모든 텍스트를 읽은 데이터는 아니고 화자에 따라 녹음된 데이터의 카테고리와 양이 차이가 있다.  
(어학 카테고리의 경우 영어가 녹음되어있으므로 사용하지 않는다. 현재는 한국어 TTS 구현을 목표로 한다.)

오디오 데이터의 총 용량은 **31.6GB**이며 녹음 시간은 **총 72시간 40분 44초**이다.

우선은 단일화자 TTS를 만드는 것을 1차 목표로 하고 있으므로 **여자1** 화자가 녹음한 데이터만을 사용하도록 한다.  
여자1 화자가 녹음한 음성 데이터의 총 길이는 **총 11시간 46분 3초**로 양은 충분하다.

## Data Sample

여자1 화자가 읽은 동화1 스크립트의 첫번째 텍스트와 오디오를 확인해보자.

Text: **다람쥐와 호랑이**  
Speech:  
<audio controls>   
    <source src="/assets/img/posts/TTS_1/1.wav" type="audio/wav">   
    Your browser does not support the audio element.   
</audio>

앞으로 모든 설명에 해당 텍스트와 오디오를 이용한다.

# Text

우선은 Text를 Input으로 넣어주기 위해 Embedding 과정을 거쳐야할 것은 분명하다. '다람쥐와 호랑이'를 그대로 Input으로 넣는 것은 당연히 불가능하다.

그러면 그 전에 Tokenizing 과정을 거쳐야하는데 이전 챗봇을 만들때와 마찬가지로 단어 임베딩을 하면 될까?  
당연히 NO이다. TTS 혹은 STT에서는 대부분 `Syllables Tokenizer`를 사용한다. 한국말로 하자면 음절 단위로 토큰화 한다라고 생각하면 된다.

현재 목적은 한국말을 TTS로 구현하는 것이므로 한국말을 음절단위로 분리해야한다.  

## Jamo Tokenizing

한국어 특유의 조사와 단어변형 등 때문에 대부분의 자연어 처리 task에서 곤욕을 겪곤 한다. 하지만 이와 대비되게 한글은 음절단위로 자르는 것이 굉장히 간단하다.  
방법은 초성, 중성, 종성을 전부 분리하면 된다.

`jamo`라이브러리를 사용하기만 하면 되므로 구현도 굉장히 간단하다.

```python
from jamo import hangul_to_jamo

jamo_tokens = list(hangul_to_jamo('다람쥐와 호랑이'))
print(f'jamo tokens:\n{jamo_tokens}\n')
print(f'inversed: {"".join(jamo_tokens)}')
```

```
jamo tokens:
['ᄃ', 'ᅡ', 'ᄅ', 'ᅡ', 'ᆷ', 'ᄌ', 'ᅱ', 'ᄋ', 'ᅪ', ' ', 'ᄒ', 'ᅩ', 'ᄅ', 'ᅡ', 'ᆼ', 'ᄋ', 'ᅵ']

inversed: 다람쥐와 호랑이
```

다음과 같이 Tokenizing을 하면 된다. 아주 쉽다.

## Vocabulary Dictionary

### Jamo Embedding

이제 Vocabulary Dictionary를 만들어 Embedding을 진행해야한다.  
딕셔너리를 만드는 방법도 굉장히 간단하다. 초성, 중성, 종성에 사용되는 모든 Token들을 모아 만들면 된다.

흔히 이런식으로 만든다.

```python
JAMO_LEADS = [chr(_) for _ in range(0x1100, 0x1113)]
JAMO_VOWELS = [chr(_) for _ in range(0x1161, 0x1176)]
JAMO_TAILS = [chr(_) for _ in range(0x11A8, 0x11C3)]

JAMOS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS
print(JAMOS)
```

```
['ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', ..., 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', ..., 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', ..., 'ᇀ', 'ᇁ', 'ᇂ']
```

이렇게 초성, 중성, 종성을 모두 표현할 수 있다.

### Text Normalization

이것만 딕셔너리에 넣으면 될까?  
당연히 안된다. 이유는 위에 '다람쥐와 호랑이'만 봐도 알 수 있다.

이유는 바로 공백. 공백 뿐만 아니라 문자에는 마침표, 쉼표, 따옴표, 물음표, 느낌표 등 다양한 기호들이 포함 된다. 이를 딕셔너리에 포함해야한다.

또한 어떤 텍스트를 사용하느냐에 따라 숫자, 알파벳, 한자 까지도 포함 될 수 있다.  
이를 두가지 방법으로 처리할 수 있는데  
첫번째 방법은 '모두 딕셔너리에 포함시킨다.'  
두번째 방법은 '모두 한글로 바꾼다.'

그리고 현재는 두가지 방법을 적절히 섞어 사용하도록 하자.

우선은 스크립트를 전부 확인하며 초성, 중성, 종성과 기본적인 기호들(마침표, 쉼표, 느낌표, 물음표, 따옴표)을 제외하고 어떤 문자들이 포함되어 있는지를 확인해야한다.

확인 결과 각종 알파벳, quotation mark, 기호, 숫자 등이 활용되었다.  
이들 중 `['-', '~', '…']`는 자체적인 특성을 가진다고 생각해 딕셔너리에 포함시켰다.  
나머지는 딕셔너리에 있는 토큰들로 대체하기 위해 다음과 같이 처리했다.

```python
def normalize_etc(text):
    text = re.sub('%', '퍼센트', text)
    text = re.sub('to', '투', text)
    text = re.sub('[·”]', ' ', text)
    return text


def normalize_quote(text):
    return re.sub('[<>‘’〈〉『』]', '\'', text)


def normalize_en(text):
    
    en_to_kor_dict = {
            'A': '에이', 'a': '에이',
            'B': '비', 'b': '비',
            'C': '씨', 'c': '씨',
            'D': '디', 'd': '디',
            'E': '이', 'e': '이',
            'F': '에프', 'f': '에프',
            'G': '지', 'g': '지',
            'H': '에이치', 'h': '에이치',
            'I': '아이', 'i': '아이',
            'J': '제이', 'j': '제이',
            'K': '케이', 'k': '케이',
            'L': '엘', 'l': '엘',
            'M': '엠', 'm': '엠',
            'N': '엔', 'n': '엔',
            'O': '오', 'o': '오',
            'P': '피', 'p': '피',
            'Q': '큐', 'q': '큐',
            'R': '알', 'r': '알',
            'S': '에스', 's': '에스',
            'T': '티', 't': '티',
            'U': '유', 'u': '유',
            'V': '브이', 'v': '브이',
            'W': '더블유', 'w': '더블유',
            'X': '엑스', 'x': '엑스',
            'Y': '와이', 'y': '와이',
            'Z': '지', 'z': '지',
    }

    return re.sub('[A-Za-z]', lambda x: en_to_kor_dict[x.group()], text)


def int_to_kor_under_4d(num):
    
    num = str(num)
    if num in ['0', '00', '000', '0000']:
        return ''
    num_to_kor_list = [''] + list('일이삼사오육칠팔구')
    unit_to_kor_list = [''] + list('십백천')
    
    result = ''
    i = len(num)
    for n in num:
        i -= 1
        n = int(n)
        str_unit = unit_to_kor_list[i]
        if str_unit != '' and n == 1:
            str_n = ''
        else:
            str_n = num_to_kor_list[n]
        result += str_n+str_unit
    return result


def int_to_kor(num):
    
    result = ''
    
    unit_to_kor_list = [''] + list('만억조경해')
    
    num = str(num)
    n_digit = len(num)
    i, d = n_digit%4, n_digit//4
    if i:
        result += int_to_kor_under_4d(num[:i])+unit_to_kor_list[d]
    d -= 1
    while i+4 <= n_digit:
        n = num[i:i+4]
        result += int_to_kor_under_4d(n)+unit_to_kor_list[d]
        i += 4
        d -= 1
        
    if result[:2] == '일만':
        result = result[1:]
        
    return result


def num_to_kor(num):
    
    num = str(num)
    if num == '0':
        return '영'
    num_to_kor_list = list('영일이삼사오육칠팔구')

    if '.' in num:
        _int, _float = num.split('.')
        float_to_kor_result = ''
        for f in _float:
            float_to_kor_result += num_to_kor_list[int(f)]
        float_to_kor_result = '쩜'+float_to_kor_result
    else:
        _int = num
        float_to_kor_result = ''
    assert len(_int) <= 24, 'Too long number'
    
    int_to_kor_result = int_to_kor(_int)
    
    return int_to_kor_result+float_to_kor_result


def normalize_num(text):
    return re.sub('\d+[.]{0,1}\d*', lambda x: num_to_kor(x.group()), text)


def normalize_text(text):
    text = normalize_etc(text)
    text = normalize_quote(text)
    text = normalize_en(text)
    text = normalize_num(text)
    return text
```

이렇게 특수문자나 알파벳 혹은 숫자를 한글로 바꿀때는 주의가 많이 필요하다. '%'도 '퍼센트', '프로'등으로 읽을 수 있고, 알파벳을 한글자씩 읽는 단어인지, 영어를 통째로 외래어처럼 사용하는 단어인지에 따라 처리하는 방법이 달라진다.  
한글로 바꿀 생각이라면 당연히 읽는 발음 그대로 바꿔주어야한다.

특수문자나 알파벳보다 숫자가 가장 큰 문제가 되는 경우가 많다. 특히 서수때문에 곤란해지는 경우가 많다.  
예를들어 '30원', '30살' 두개를 읽을 때, '삼십원', '서른살'처럼 읽는 방법이 다르다.  
하지만 정말 감사하게도 현재 사용하는 스크립트에는 '서른' 뿐만 아니라 '하나', '둘' 처럼 기수가 아닌 서수 형태로 숫자를 읽는 경우에는 발음 그대로 한글로 표기 되어있다.  
따라서 숫자는 모두 기수 형태로만 처리해주었다.

### \<SOS\> \<EOS\> \<PAD\> Token

\<SOS\> \<EOS\> \<PAD\> Token은 앞으로 딕셔너리에 포함되지 않을 아무거나 사용하면 된다. 뭐가 됐든 별로 상관 없다.  
각각 `['@', '|', '_']`를 사용해주도록 하자.

Vocaburaly Dictionary를 완성했다. 이제 이를 이용하여 Tokenizing을 해주면 된다.

## Unmatched Data

텍스트 전처리를 마쳤다고 생각하며 데이터를 살펴보던 중 이상한 것을 발견했다.

우선 데이터가 어떻게 생겼는지를 보자면.

![script_image](/assets/img/posts/TTS_1/script_image.png)
![wav_folder_image](/assets/img/posts/TTS_1/wav_folder_image.png)

오디오 파일은 `화자_카테고리`형태로 폴더가 구분되어있고 그 안에 해당 스크립트의 `Index`번호에 맞는 오디오가 녹음되어있다. 

**그런데...!** 번호가 맞지 않는 오디오가 숨어있다...?

몇개 랜덤하게 오디오파일을 열어 확인해보니 스크립트와 일치하지 않는 오디오가 녹음되어있는 경우가 숨어있다... 한두칸씩 밀리거나 당겨 녹음된 파일들이 꽤나 있다... 큰일났다...

처음 이걸 발견하고 정말 헛웃음이 나왔다...ㅋㅋㅋㅋㅋ 이걸 어떻게 처리해야하나 고민을 많이 해봤다.  
***하나하나 열어서 확인해야하나..?*** 이건 말이 안된다. 데이터는 총 72시간이 넘는다.  
***데이터셋을 바꿔야하나..?*** 하나의 방법이 되겠지만 결국 그 데이터도 완벽한지 확인이 필요하다. 그리고 현재 데이터의 구조가 맘에든다.  
***그럼 어떡하지..?***

그래서 떠올린 약간은 야매스러운? 방법.  
**STT 모듈을 하나 이용하자.**

아이디어는 이러하다.

- 모든 오디오파일에 대해 STT결과와 원본 텍스트를 비교한다.
- 결과 비교에는 BLEU Score를 사용한다.
- 특정 Threshold에 대해 BLEU Score가 이를 넘지 못하면 logging을 남긴다.
- log를 확인하여 아예 엉뚱한 오디오는 지우고, 녹음이 한두칸 당겨지거나 밀린경우 오디오 파일의 파일명을 바꿔 스크립트와 맞게 바꾼다.

### STT

`speech_recognition`을 사용하여 간단하게 STT를 수행할 수 있다.  
정말 감사하게도 한국어를 지원한다.  
이를 사용하여 결과를 확인해보자.

```python
import speech_recognition as sr

def get_stt_result(r, fpath):
    korean_audio = sr.AudioFile(fpath)

    with korean_audio as source:
        audio = r.record(source)
    result = r.recognize_google(audio_data=audio, language='ko-KR')
    return result

r = sr.Recognizer()
fpath = '../data/wav/여1_동화1/1.wav'  # 다람쥐와 호랑이
stt_result = get_stt_result(r, fpath)
print(f'STT result: {stt_result}')
```

```
STT result: 다람쥐와 호랑이
```

오호라.. 완벽하다. 계획대로 진행하면 되겠다. (사실 모든 문장에 대해 완벽한 정확도를 자랑하지는 않지만 상당히 정확한편이다.)

### BLEU Score

BLEU(Bilingual Evaluation Understudy)score는 N-gram 기반으로 동작하는 문장 유사도를 측정하는 점수이다.  

BLEU Score에 대해 설명하는 포스팅은 아니므로 정말 간단하게만 설명한다.  
문장의 토큰들을 1부터 4의 길이를 가지는 window로 옮겨가며 같은 형태를 가진 window가 얼마나 함께 등장하는지를 측정하는 것이다.  
그리고 이 과정에서 문장 길이에 대한 보정과 같은 단어가 연속적으로 나오는 것에 대한 보정을 거친다.

좀 더 자세한 설명은 [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31695)에 잘 정리되어있으므로 참고하도록하자.

다음 두 문장을 비교해보도록 하자.

**'물고기를 잔뜩 먹게 해준단 말이야?'**  
**'물꼬기를 잔득 먹게 해준다는 말이야?'**

```python
from jamo import hangul_to_jamo
from nltk.translate import bleu_score

text1 = '물고기를 잔뜩 먹게 해준단 말이야?'
text2 = '물꼬기를 잔득 먹게 해준다는 말이야?'

text1_tokens = list(hangul_to_jamo(text1))
text2_tokens = list(hangul_to_jamo(text2))

score = bleu_score.sentence_bleu([text1_tokens], text2_tokens)
print(f'BLEU Score: {score:.6f}')
```

```
BLEU Score: 0.788087
```

다음과 같이 사용할 수 있다. 1에 가까울 수록 일치하는 문장이며 완벽이 같은 문장은 1이 나온다.

현재 BLEU Score의 계산 목적은 BLEU Score 점수 자체에 있지 않다.  
그냥 제대로 된 오디오가 녹음되어 있는지, 스크립트와 일치하는지를 비교하는 것이 목적이다.  
따라서 실제 작성하여 사용한 코드는 위와는 조금 다르다.

앞에 정의한 `normalize_text`를 거친뒤, 띄어쓰기와 기호는 모두 제거했다.  
이에 1~4 gram 기반의 점수를 모두 사용하지 않고 1과 2만을 사용했다.

실제 작성한 코드는 [링크](https://github.com/fidabspd/tts/blob/master/codes/match_text_speech.py)를 참고.

그렇게 나온 로그.

```
2022-03-25 20:26:46	INFO	speech_path: '../data/wav/여1_소설1/489.wav'
2022-03-25 20:26:46	INFO	bleu score: 0.25067
2022-03-25 20:26:46	INFO	origin_text:	더이상욕보이지말고빨리죽여
2022-03-25 20:26:46	INFO	stt_result:		이래도못알아보면서개뿔

2022-03-25 20:26:46	INFO	speech_path: '../data/wav/여1_소설1/490.wav'
2022-03-25 20:26:46	INFO	bleu score: 0.26976
2022-03-25 20:26:46	INFO	origin_text:	이래도못알아보면서개뿔
2022-03-25 20:26:46	INFO	stt_result:		못이긴뭘못잊어

2022-03-25 20:26:47	INFO	speech_path: '../data/wav/여1_소설1/491.wav'
2022-03-25 20:26:47	INFO	bleu score: 0.17849
2022-03-25 20:26:47	INFO	origin_text:	못잊긴뭘못잊어
2022-03-25 20:26:47	INFO	stt_result:		이제기억나

2022-03-25 20:26:47	INFO	speech_path: '../data/wav/여1_소설1/492.wav'
2022-03-25 20:26:47	INFO	bleu score: 0.17078
2022-03-25 20:26:47	INFO	origin_text:	이제기억나
2022-03-25 20:26:47	INFO	stt_result:		정말로아가씨야
```

이렇게 한두칸씩 밀리거나 당겨진채 녹음된 오디오를 확인할 수 있다. 이제 하나하나 열어보지 않아도  녹음이 잘못된 파일을 구분할 수 있다..! ~~물론 아예 열어보지 않을 수는 없다.~~

이렇게 녹음이 잘못된파일을 지우거나 파일명을 바꿔 Text-Speech의 쌍을 맞춰주는 작업을 진행했다.  
물론 파일명 바꿀때 하나하나 바꾸진 않고 코드를 작성하여 사용했다. 이는 간단하므로 패스.

한번 Text-Speech 쌍을 맞춘 뒤 한번더 STT를 통한 일치를 비교하여 로깅을 남기고 더블체크를 하여 Data Cleansing을 마쳤다.

## 마무리

이제 진짜로 텍스트 관련 처리와 데이터 Data Cleansing은 끝났다.

다음 포스팅에서는 텍스트 처리에 이어 오디오 관련 처리를 진행해보자.
